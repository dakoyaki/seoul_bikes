{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "seoul_bikes.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "EgUvIS54fG4u",
        "wS2EYtzBfHTk"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiixLz2xfG0H"
      },
      "source": [
        "# Bike Sharing Demand"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynhEdNWhfG09"
      },
      "source": [
        "###Data Understanding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N2vZ3W8fG0_",
        "outputId": "c1d29101-54de-457e-b118-3292c9266c3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install skits\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import pickle\n",
        "from scipy.stats import kruskal, pearsonr, randint, uniform, chi2_contingency, boxcox\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, FunctionTransformer, StandardScaler, power_transform\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error\n",
        "from sklearn.model_selection import cross_val_score, cross_validate, TimeSeriesSplit, RandomizedSearchCV, GridSearchCV, cross_val_predict\n",
        "from datetime import datetime\n",
        "from statsmodels.tsa.stattools import grangercausalitytests, adfuller, kpss, acf, pacf\n",
        "from collections import defaultdict, OrderedDict\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
        "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
        "from sklearn.decomposition import PCA\n",
        "from statsmodels.tsa.ar_model import AR\n",
        "from skits.feature_extraction import AutoregressiveTransformer\n",
        "from skits.preprocessing import ReversibleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import xgboost as xgb\n",
        "\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: skits in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from skits) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from skits) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->skits) (0.17.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->skits) (1.4.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbXB3v-_fG1D"
      },
      "source": [
        "# diplaying all columns without truncation in dataframes\n",
        "pd.set_option('display.max_columns', 500)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s3YvZmxfG1M"
      },
      "source": [
        "#### Loading the data and visualizing and describing it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "oJy-rTgJfG1U"
      },
      "source": [
        "# read in bike sharing dataset\n",
        "bike_df = pd.read_csv('/content/bike_sharing_dataset.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECkD1nPqfG35"
      },
      "source": [
        "# fill NAs with 0 where applicable\n",
        "bike_df['holiday'] = bike_df['holiday'].fillna(0)\n",
        "bike_df['holiday'] = bike_df['holiday'].astype('int')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oyAEfvYfG4V"
      },
      "source": [
        "# function to create seasons for dataframe\n",
        "def seasons(df):\n",
        "   # create a season features\n",
        "    df['season_spring'] = df['Date'].apply(lambda x: 1 if '03' in x[5:7] else 1 if '04' in x[5:7] else 1 \n",
        "                                                     if '05' in x[5:7] else 0)\n",
        "    df['season_summer'] = df['Date'].apply(lambda x: 1 if '06' in x[5:7] else 1 if '07' in x[5:7] else 1 \n",
        "                                                     if '08' in x[5:7] else 0)\n",
        "    df['season_fall'] = df['Date'].apply(lambda x: 1 if '09' in x[5:7] else 1 if '10' in x[5:7] else 1 \n",
        "                                                     if '11' in x[5:7] else 0)\n",
        "    df['season_winter'] = df['Date'].apply(lambda x: 1 if '12' in x[5:7] else 1 if '01' in x[5:7] else 1 \n",
        "                                                     if '02' in x[5:7] else 0)\n",
        "    \n",
        "    return df\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqzbZ7MEfG4X"
      },
      "source": [
        "### create new features for seasons\n",
        "bike_df = seasons(bike_df)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkqz0141fG4a"
      },
      "source": [
        "### create new feature weekday\n",
        "bike_df['date_datetime'] = bike_df['Date'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))\n",
        "bike_df['weekday'] = bike_df['date_datetime'].apply(lambda x: x.weekday())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R2o4uLufG4j"
      },
      "source": [
        "### one hot encode the feature weekday\n",
        "## 0 = Monday, 1 = Tuesday, 2 = Wednesday, etc...\n",
        "weekday_dummies = pd.get_dummies(bike_df['weekday'], prefix='weekday', drop_first=True)\n",
        "bike_df = bike_df.join(weekday_dummies, how='left')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJlqxtHpfG4l"
      },
      "source": [
        "### create new feature working_day\n",
        "bike_df['working_day'] = bike_df['weekday'].apply(lambda x: 0 if x > 5 or x == 0 else 1)\n",
        "bike_df['working_day'] = bike_df[['holiday', 'working_day']].apply(\n",
        "    lambda x: 0 if x['holiday'] == 1 else x['working_day'], axis=1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "spfyG3L_fG4s"
      },
      "source": [
        "# drop columns that are irrelevant\n",
        "bike_df.drop(columns=['City', 'ID', 'DistrictKOR', 'DistrictENG', \n",
        "                      'Area(m2)','Bike_Time_NONZERO_SUM',\t'Bike_Time_NONZERO_Mean',\t\n",
        "                      'Bike_Distance_NONZERO_SUM',\t'Bike_Distance_NONZERO_Mean', \n",
        "                      'Trip_Count_NONZERO', 'Bike_Time_SUM', 'BIKE_TIME_MEAN', \n",
        "                      'Bike_Distance_SUM', 'BIKE_DISTANCE_MEAN'], inplace=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWWZDZcqfG8T"
      },
      "source": [
        "# create a new dataframe that encodes the weekday feature with 0 for monday through friday\n",
        "# and 1 for saturday and sunday\n",
        "weekend_distinct_df = bike_df.copy()\n",
        "weekend_distinct_df['weekday'] = weekend_distinct_df['weekday'].apply(lambda x: 1 if (x == 6 or x == 0) else 0)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkrnA_PMHYUV"
      },
      "source": [
        "bike_df.drop(columns=['WinddirectM(deg)'], inplace=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vnhF3ikfHCv"
      },
      "source": [
        "# get list for all correlations between a feature and Trip_Count with different rolling means\n",
        "def best_window(x, y, max_window):\n",
        "    corr_temp_cust = []\n",
        "    for i in range(1, max_window):\n",
        "        roll_val = list(x.rolling(i).mean()[i-1:-1])\n",
        "        Trip_Count_ti = list(y[i:])\n",
        "        corr, p_val = pearsonr(Trip_Count_ti, roll_val)\n",
        "        corr_temp_cust.append(corr)\n",
        "    # get the optimal window size for rolling mean between a feature and Trip_Count\n",
        "    max_val = np.argmax(corr_temp_cust)\n",
        "    min_val = np.argmin(corr_temp_cust)\n",
        "    opt_corr_min = corr_temp_cust[min_val]\n",
        "    opt_corr_max = corr_temp_cust[max_val]\n",
        "    \n",
        "    results = {max_val+1: opt_corr_max, min_val+1: opt_corr_min}\n",
        "    \n",
        "    return results\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j8sGCyEfHFb"
      },
      "source": [
        "# get list for all correlations between a feature and Trip_Count with different rolling standard deviations\n",
        "def best_window_std(x, y, max_window):\n",
        "    corr_temp_cust = []\n",
        "    for i in range(2, max_window):\n",
        "        roll_val = list(x.rolling(i).std()[i-1:-1])\n",
        "        Trip_Count_ti = list(y[i:])\n",
        "        corr, p_val = pearsonr(Trip_Count_ti, roll_val)\n",
        "        corr_temp_cust.append(corr)\n",
        "    # get the optimal window size for rolling std between a feature and Trip_Count\n",
        "    max_val = np.argmax(corr_temp_cust)\n",
        "    min_val = np.argmin(corr_temp_cust)\n",
        "    opt_corr_min = corr_temp_cust[min_val]\n",
        "    opt_corr_max = corr_temp_cust[max_val]\n",
        "    \n",
        "    results = {max_val+1: opt_corr_max, min_val+1: opt_corr_min}\n",
        "    \n",
        "    return results\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVzkAGjyfHGg",
        "outputId": "924d1505-8c86-4528-d0cb-ded12752f733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# get the optimal window for rolling std for Trip_Count\n",
        "print(best_window_std(bike_df['Trip_Count'], bike_df['Trip_Count'], 40))\n",
        "\n",
        "# get the correlation for window size determined by Trip_Count\n",
        "cust_mean = bike_df['Trip_Count'].rolling(8).std()[7:-1]\n",
        "pearsonr(cust_mean, bike_df['Trip_Count'][8:])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{21: 0.7458671048146939, 1: 0.3185045110199073}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5909874097580865, 0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP0uUwbZfHGp",
        "outputId": "1b20340a-c8c8-4112-f964-b0d79f1a72b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# get the optimal number for rolling mean for Trip_Count\n",
        "print(best_window(bike_df['Trip_Count'], bike_df['Trip_Count'], 40))\n",
        "\n",
        "# get the correlation for window size determined by Trip_Count\n",
        "cust_mean = bike_df['Trip_Count'].rolling(8).mean()[7:-1]\n",
        "pearsonr(cust_mean, bike_df['Trip_Count'][8:])\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{7: 0.8878801976300186, 39: 0.8177747047009136}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8863948653212457, 0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqgeoPs8fHGs"
      },
      "source": [
        "# add the value from t-1\n",
        "bike_df['Trip_Count_t-1'] = bike_df['Trip_Count'].shift()\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfARS8k1fHGw"
      },
      "source": [
        "# create series that group the mean temperature per season\n",
        "temp_spring = bike_df.groupby('season_spring')['TempM(°C)'].mean().rename({1: 'Spring'})\n",
        "temp_summer = bike_df.groupby('season_summer')['TempM(°C)'].mean().rename({1: 'Summer'})\n",
        "temp_fall = bike_df.groupby('season_fall')['TempM(°C)'].mean().rename({1: 'Fall'})\n",
        "temp_winter = bike_df.groupby('season_winter')['TempM(°C)'].mean().rename({1: 'Winter'})\n",
        "\n",
        "# add them to one series and drop the rows with index 0\n",
        "temp_seasons = temp_summer.append(temp_fall).append(temp_winter).append(temp_spring)\n",
        "temp_seasons.drop(labels=[0], inplace=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSXWES_ofHHv"
      },
      "source": [
        "# create series that groups average users per season\n",
        "cust_spring = bike_df.groupby('season_spring')['Trip_Count'].mean().rename({1: 'Spring'})\n",
        "cust_summer = bike_df.groupby('season_summer')['Trip_Count'].mean().rename({1: 'Summer'})\n",
        "cust_fall = bike_df.groupby('season_fall')['Trip_Count'].mean().rename({1: 'Fall'})\n",
        "cust_winter = bike_df.groupby('season_winter')['Trip_Count'].mean().rename({1: 'Winter'})\n",
        "\n",
        "# add them to one series and drop the rows with index 0\n",
        "cust_seasons = cust_summer.append(cust_fall).append(cust_winter).append(cust_spring)\n",
        "cust_seasons.drop(labels=[0], inplace=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jR9JcMxfHJL",
        "outputId": "3b1567e8-586c-47ec-b83a-bacd6e90afa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# get the optimal window for rolling std for temperature\n",
        "print(best_window_std(bike_df['TempM(°C)'], bike_df['Trip_Count'], 40))\n",
        "\n",
        "# get the correlation for window size determined by TempM(°C)\n",
        "temp_mean = bike_df['TempM(°C)'].rolling(8).std()[7:-1]\n",
        "pearsonr(temp_mean, bike_df['Trip_Count'][8:])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: -0.11818513201317055, 31: -0.279594519569663}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.2260233053368831, 5.0153556185029124e-141)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbVJwHnHfHJN",
        "outputId": "ac2b19a5-2e97-45c5-98ad-c810a47d15d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# get the optimal window for rolling mean for temperature\n",
        "best_window(bike_df['TempM(°C)'], bike_df['Trip_Count'], 40)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{6: 0.38647683685028433, 39: 0.3383048120182877}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDGzHFMefHJ2"
      },
      "source": [
        "# engineer new categorical AQ features\n",
        "bike_df_cat = bike_df.copy()\n",
        "bike_df_cat['PM10_good'] = bike_df_cat['PM10'].apply(lambda x: 1 if x < 30 else 0)\n",
        "bike_df_cat['PM10_moderate'] = bike_df_cat['PM10'].apply(lambda x: 1 if x < 80 and x >= 31 else 0)\n",
        "bike_df_cat['PM10_unhealthy'] = bike_df_cat['PM10'].apply(lambda x: 1 if x < 150 and x >= 81 else 0)\n",
        "bike_df_cat['PM10_very_unhealthy'] = bike_df_cat['PM10'].apply(lambda x: 1 if x >= 151 else 0)\n",
        "bike_df_cat['PM25_good'] = bike_df_cat['PM2.5'].apply(lambda x: 1 if x < 15 else 0)\n",
        "bike_df_cat['PM25_moderate'] = bike_df_cat['PM2.5'].apply(lambda x: 1 if x < 35 and x >= 16 else 0)\n",
        "bike_df_cat['PM25_unhealthy'] = bike_df_cat['PM2.5'].apply(lambda x: 1 if x < 75 and x >= 36 else 0)\n",
        "bike_df_cat['PM25_very_unhealthy'] = bike_df_cat['PM2.5'].apply(lambda x: 1 if x >= 76 else 0)\n",
        "\n",
        "bike_df_cat[['PM10_good', 'PM10_moderate', 'PM10_unhealthy', 'PM10_very_unhealthy', 'PM25_good', 'PM25_moderate', 'PM25_unhealthy', 'PM25_very_unhealthy']] = bike_df_cat[['PM10_good', 'PM10_moderate', 'PM10_unhealthy', 'PM10_very_unhealthy', 'PM25_good', 'PM25_moderate', 'PM25_unhealthy', 'PM25_very_unhealthy']].shift()\n",
        "\n",
        "bike_df_cat = bike_df_cat.iloc[1:,:]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiRi_2TvfHKQ",
        "outputId": "f9e2d60a-fcd3-43e6-ef41-3f0b87971128",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# get the optimal window for rolling std for RainM(mm)\n",
        "print(best_window_std(bike_df['RainM(mm)'], bike_df['Trip_Count'], 40))\n",
        "\n",
        "# get the correlation for window size determined by RainM(mm)\n",
        "precip_mean = bike_df['RainM(mm)'].rolling(8).std()[7:-1]\n",
        "pearsonr(precip_mean, bike_df['Trip_Count'][8:])\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{38: 0.061215418070146466, 2: -0.021762566657129716}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.0003705742186305601, 0.9673647401483304)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RKlDIpmfHMg",
        "outputId": "7e5cb2d0-ed5e-45f4-88ad-a283f274fead",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# get the optimal number for rolling mean window\n",
        "print(best_window(bike_df['WindSpeedM(m/s)'], bike_df['Trip_Count'], 40))\n",
        "\n",
        "# get the correlation for window size determined by wind\n",
        "wind_mean = bike_df['WindSpeedM(m/s)'].rolling(8).mean()[7:-1]\n",
        "pearsonr(wind_mean, bike_df['Trip_Count'][8:])[0]\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{39: 0.023631717822896525, 1: -0.026152704864007892}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.0026884178670223413"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GIWLiZtfHMl",
        "outputId": "d18c50da-d577-4ca0-c40c-5a89f83a2bdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# get the optimal window for rolling std for temperature\n",
        "print(best_window_std(bike_df['WindSpeedM(m/s)'], bike_df['Trip_Count'], 40))\n",
        "\n",
        "# get the correlation for window size determined by wind\n",
        "wind_mean = bike_df['WindSpeedM(m/s)'].rolling(8).std()[7:-1]\n",
        "pearsonr(wind_mean, bike_df['Trip_Count'][8:])\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{38: -0.05679909742989022, 6: -0.11929895283492986}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.11382634433058314, 1.9049735545374817e-36)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VuhPK4BKHRY"
      },
      "source": [
        "ENTROPYBIG_mean = bike_df['ENTROPYBIG'].rolling(8).mean()[7:-1]\n",
        "ENTROPYSMALL_mean = bike_df['ENTROPYSMALL'].rolling(8).mean()[7:-1]\n",
        "RESIDENTIAL110_mean = bike_df['110_RESIDENTIAL'].rolling(8).mean()[7:-1]\n",
        "HOUSE111_mean = bike_df['111_HOUSE'].rolling(8).mean()[7:-1]\n",
        "APT112_mean = bike_df['112_APT'].rolling(8).mean()[7:-1]\n",
        "INDUSTRIAL120_mean = bike_df['120_INDUSTRIAL'].rolling(8).mean()[7:-1]\n",
        "INDUSTRIAL121_mean = bike_df['121_INDUSTRIAL'].rolling(8).mean()[7:-1]\n",
        "COMMERCIAL130_mean = bike_df['130_COMMERCIAL'].rolling(8).mean()[7:-1]\n",
        "COMMERCIAL131_WORK_mean = bike_df['131_COMMERCIAL_WORK'].rolling(8).mean()[7:-1]\n",
        "MIXEDUSE132_mean = bike_df['132_MIXEDUSE'].rolling(8).mean()[7:-1]\n",
        "RECREATIONAL140_mean = bike_df['140_RECREATIONAL'].rolling(8).mean()[7:-1]\n",
        "RECREATIONAL141_mean = bike_df['141_RECREATIONAL'].rolling(8).mean()[7:-1]\n",
        "TRANSIT150_mean = bike_df['150_TRANSIT'].rolling(8).mean()[7:-1]\n",
        "PUBLIC160_mean = bike_df['160_PUBLIC'].rolling(8).mean()[7:-1]\n",
        "GREENINFRA_mean = bike_df['GREENINFRA'].rolling(8).mean()[7:-1]\n",
        "POPDensity_mean = bike_df['POPDensity(p/㎢)'].rolling(8).mean()[7:-1]\n",
        "POPGender_mean = bike_df['POPGender(Male/Female)'].rolling(8).mean()[7:-1]\n",
        "Age_under24_mean = bike_df['Age_under24'].rolling(8).mean()[7:-1]\n",
        "PM10_mean = bike_df['PM10'].rolling(8).mean()[7:-1]\n",
        "PM25_mean = bike_df['PM2.5'].rolling(8).mean()[7:-1]\n",
        "COVID_mean = bike_df['COVID'].rolling(8).mean()[7:-1]\n",
        "Metro_station_mean = bike_df['Metro_station'].rolling(8).mean()[7:-1]\n",
        "Metrotrip18_IN_mean = bike_df['18Metrotrip_IN'].rolling(8).mean()[7:-1]\n",
        "Metrotrip18_OUT_mean = bike_df['18Metrotrip_OUT'].rolling(8).mean()[7:-1]\n",
        "Metrotrip_IN_mean = bike_df['Metrotrip_IN'].rolling(8).mean()[7:-1]\n",
        "Metrotrip_OUT_mean = bike_df['Metrotrip_OUT'].rolling(8).mean()[7:-1]\n",
        "Bus_station_mean = bike_df['Bus_station'].rolling(8).mean()[7:-1]\n",
        "BusINCount_mean = bike_df['BusINCount'].rolling(8).mean()[7:-1]\n",
        "BusOUTCount_mean = bike_df['BusOUTCount'].rolling(8).mean()[7:-1]\n",
        "Bike_station_mean = bike_df['Bike_station'].rolling(8).mean()[7:-1]\n",
        "Bike_stand_mean = bike_df['Bike_stand'].rolling(8).mean()[7:-1]\n",
        "BikeRoadCount_mean = bike_df['BikeRoadCount(개)'].rolling(8).mean()[7:-1]\n",
        "BikeRoadDistance_mean = bike_df['BikeRoadDistance(km)'].rolling(8).mean()[7:-1]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMYDMXypfHM4"
      },
      "source": [
        "# code based on implementation on https://www.analyticsvidhya.com/blog/2018/09/non-stationary-time-series-python/\n",
        "def adf_test(df, col_names):\n",
        "    '''\n",
        "    Function to perform Augmented Dickey-Fuller test on selected timeseries\n",
        "    Args: df = dataframe with timeseries to be tested\n",
        "          col_names = list of names of the timeseries to be tested\n",
        "    Returns: None\n",
        "    '''\n",
        "    for name in col_names:\n",
        "        print ('Results of Augmented Dickey-Fuller Test for {}'.format(name))\n",
        "        result_test = adfuller(df[name], autolag='AIC')\n",
        "        result_output = pd.Series(result_test[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
        "        for key, val in result_test[4].items():\n",
        "            result_output['Critical Value (%s)'%key] = val\n",
        "        print (result_output)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIHcMQ_UfHM7"
      },
      "source": [
        "# create the features that need to be tested\n",
        "# Trip_Count_t-1 was already added to the dataframe\n",
        "\n",
        "testing_feat = ['Trip_Count', 'ENTROPYBIG',\t'ENTROPYSMALL',\t'110_RESIDENTIAL',\t'111_HOUSE',\t'112_APT',\t'120_INDUSTRIAL',\t'121_INDUSTRIAL',\t'130_COMMERCIAL',\t'131_COMMERCIAL_WORK',\t'132_MIXEDUSE',\t'140_RECREATIONAL',\t'141_RECREATIONAL',\t'150_TRANSIT',\t'160_PUBLIC',\t'GREENINFRA',\t'POPDensity(p/㎢)',\t'POPGender(Male/Female)',\t'Age_under24',\t'TempM(°C)',\t'WindSpeedM(m/s)',\t'RainM(mm)',\t'PM10',\t'PM2.5',\t'COVID',\t'Metro_station',\t'18Metrotrip_IN',\t'18Metrotrip_OUT',\t'Metrotrip_IN',\t'Metrotrip_OUT',\t'Bus_station',\t'BusINCount',\t'BusOUTCount',\t'Bike_station',\t'Bike_stand',\t'BikeRoadCount(개)',\t'BikeRoadDistance(km)']\n",
        "\n",
        "testing_df = pd.DataFrame()\n",
        "\n",
        "for col in testing_feat:\n",
        "    col_mean = bike_df[col].rolling(16).mean()[15:-1]\n",
        "    col_std = bike_df[col].rolling(16).std()[15:-1]\n",
        "    testing_df[col+'_mean16'] = col_mean.values\n",
        "    testing_df[col+'_std16'] = col_std.values\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4ZGhH3DfHM_",
        "outputId": "69a3e6ae-f535-4fac-fe26-3eee1ec56f38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# adf test for Trip_Count_t-1\n",
        "temp_cust_1 = bike_df['Trip_Count_t-1'].fillna(0)\n",
        "bike_df_temp = pd.DataFrame(temp_cust_1, columns=['Trip_Count_t-1'])\n",
        "adf_test(bike_df_temp, ['Trip_Count_t-1'])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results of Augmented Dickey-Fuller Test for Trip_Count_t-1\n",
            "Test Statistic                -7.513128e+00\n",
            "p-value                        3.967003e-11\n",
            "#Lags Used                     4.000000e+01\n",
            "Number of Observations Used    1.215900e+04\n",
            "Critical Value (1%)           -3.430888e+00\n",
            "Critical Value (5%)           -2.861778e+00\n",
            "Critical Value (10%)          -2.566897e+00\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeGoYRZsfHOB",
        "outputId": "cd080ab5-deb4-41a0-e1cd-b7a2015e156e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# adf test for Trip_Count\n",
        "adf_test(bike_df, ['Trip_Count'])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results of Augmented Dickey-Fuller Test for Trip_Count\n",
            "Test Statistic                -7.513455e+00\n",
            "p-value                        3.959547e-11\n",
            "#Lags Used                     4.000000e+01\n",
            "Number of Observations Used    1.215900e+04\n",
            "Critical Value (1%)           -3.430888e+00\n",
            "Critical Value (5%)           -2.861778e+00\n",
            "Critical Value (10%)          -2.566897e+00\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-PrMp1KfHPz"
      },
      "source": [
        "# code based on implementation on https://www.analyticsvidhya.com/blog/2018/09/non-stationary-time-series-python/\n",
        "def kpss_test(df, col_names):\n",
        "    '''\n",
        "    Function to perform KPSS test on selected timeseries\n",
        "    Args: df = dataframe with timeseries to be tested\n",
        "          col_names = list of names of the timeseries to be tested\n",
        "    Returns: None\n",
        "    '''\n",
        "    for name in col_names:\n",
        "        print ('Results of KPSS Test for {}'.format(name))\n",
        "        result_test = kpss(df[name], regression='c', lags='legacy')\n",
        "        result_output = pd.Series(result_test[0:3], index=['Test Statistic','p-value','Lags Used'])\n",
        "        for key, val in result_test[3].items():\n",
        "            result_output['Critical Value (%s)'%key] = val\n",
        "        print (result_output)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDwaAqbhfHQi",
        "outputId": "2ea7b25d-98ff-481a-fa08-aac0e7825696",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# kpss test for Trip_Count_t-1\n",
        "kpss_test(bike_df_temp, ['Trip_Count_t-1'])\n",
        "\n",
        "# kpss test for Trip_Count\n",
        "kpss_test(bike_df, ['Trip_Count'])\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results of KPSS Test for Trip_Count_t-1\n",
            "Test Statistic            3.787181\n",
            "p-value                   0.010000\n",
            "Lags Used                40.000000\n",
            "Critical Value (10%)      0.347000\n",
            "Critical Value (5%)       0.463000\n",
            "Critical Value (2.5%)     0.574000\n",
            "Critical Value (1%)       0.739000\n",
            "dtype: float64\n",
            "Results of KPSS Test for Trip_Count\n",
            "Test Statistic            3.780879\n",
            "p-value                   0.010000\n",
            "Lags Used                40.000000\n",
            "Critical Value (10%)      0.347000\n",
            "Critical Value (5%)       0.463000\n",
            "Critical Value (2.5%)     0.574000\n",
            "Critical Value (1%)       0.739000\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tsa/stattools.py:1709: InterpolationWarning: p-value is smaller than the indicated p-value\n",
            "  warn(\"p-value is smaller than the indicated p-value\", InterpolationWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tsa/stattools.py:1709: InterpolationWarning: p-value is smaller than the indicated p-value\n",
            "  warn(\"p-value is smaller than the indicated p-value\", InterpolationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZaYwxSIfHQk"
      },
      "source": [
        "# Trying to lessen trends (make it stationary) via log\n",
        "\n",
        "not_stationary =['ENTROPYBIG_mean16',\t'ENTROPYSMALL_mean16',\t'110_RESIDENTIAL_mean16',\t\n",
        "                 '111_HOUSE_mean16',\t'112_APT_mean16',\t'120_INDUSTRIAL_mean16',\t'121_INDUSTRIAL_mean16',\t\n",
        "                 '130_COMMERCIAL_mean16',\t'131_COMMERCIAL_WORK_mean16',\t'132_MIXEDUSE_mean16',\t'140_RECREATIONAL_mean16',\t\n",
        "                 '141_RECREATIONAL_mean16',\t'150_TRANSIT_mean16',\t'160_PUBLIC_mean16',\t'GREENINFRA_mean16',\t\n",
        "                 'POPDensity(p/㎢)_mean16',\t'POPGender(Male/Female)_mean16',\t'Age_under24_mean16',\t'WindSpeedM(m/s)_mean16',\t\n",
        "                 'Metro_station_mean16',\t'18Metrotrip_IN_mean16',\t'18Metrotrip_OUT_mean16',\t'Metrotrip_IN_mean16',\t\n",
        "                 'Metrotrip_OUT_mean16',\t'Bus_station_mean16',\t'Bike_station_mean16',\t'Bike_stand_mean16',\t\n",
        "                 'BikeRoadCount(개)_mean16',\t'BikeRoadDistance(km)_mean16',\t'Trip_Count_mean16',\t'Trip_Count_t-1_mean16',\t\n",
        "                 'WindSpeedM(m/s)_std16',\t'18Metrotrip_IN_std16',\t'18Metrotrip_OUT_std16',\t'Metrotrip_IN_std16',\t'Metrotrip_OUT_std16']\n",
        "\n",
        "bike_df_temp['Trip_Count_t-1_log'] = [np.log1p(x+1) for x in bike_df_temp['Trip_Count_t-1']]\n",
        "testing_df['ENTROPYBIG_mean16_log'] = [np.log1p(x+1) for x in testing_df['ENTROPYBIG_mean16']]\n",
        "testing_df['ENTROPYSMALL_mean16_log'] = [np.log1p(x+1) for x in testing_df['ENTROPYSMALL_mean16']]\n",
        "testing_df['110_RESIDENTIAL_mean16_log'] = [np.log1p(x+1) for x in testing_df['110_RESIDENTIAL_mean16']]\n",
        "testing_df['111_HOUSE_mean16_log'] = [np.log1p(x+1) for x in testing_df['111_HOUSE_mean16']]\n",
        "testing_df['112_APT_mean16_log'] = [np.log1p(x+1) for x in testing_df['112_APT_mean16']]\n",
        "testing_df['120_INDUSTRIAL_mean16_log'] = [np.log1p(x+1) for x in testing_df['120_INDUSTRIAL_mean16']]\n",
        "testing_df['121_INDUSTRIAL_mean16_log'] = [np.log1p(x+1) for x in testing_df['121_INDUSTRIAL_mean16']]\n",
        "testing_df['130_COMMERCIAL_mean16_log'] = [np.log1p(x+1) for x in testing_df['130_COMMERCIAL_mean16']]\n",
        "testing_df['131_COMMERCIAL_WORK_mean16_log'] = [np.log1p(x+1) for x in testing_df['131_COMMERCIAL_WORK_mean16']]\n",
        "testing_df['132_MIXEDUSE_mean16_log'] = [np.log1p(x+1) for x in testing_df['132_MIXEDUSE_mean16']]\n",
        "testing_df['140_RECREATIONAL_mean16_log'] = [np.log1p(x+1) for x in testing_df['140_RECREATIONAL_mean16']]\n",
        "testing_df['141_RECREATIONAL_mean16_log'] = [np.log1p(x+1) for x in testing_df['141_RECREATIONAL_mean16']]\n",
        "testing_df['150_TRANSIT_mean16_log'] = [np.log1p(x+1) for x in testing_df['150_TRANSIT_mean16']]\n",
        "testing_df['160_PUBLIC_mean16_log'] = [np.log1p(x+1) for x in testing_df['160_PUBLIC_mean16']]\n",
        "testing_df['GREENINFRA_mean16_log'] = [np.log1p(x+1) for x in testing_df['GREENINFRA_mean16']]\n",
        "testing_df['POPDensity(p/㎢)_mean16_log'] = [np.log1p(x+1) for x in testing_df['POPDensity(p/㎢)_mean16']]\n",
        "testing_df['POPGender(Male/Female)_mean16_log'] = [np.log1p(x+1) for x in testing_df['POPGender(Male/Female)_mean16']]\n",
        "testing_df['Age_under24_mean16_log'] = [np.log1p(x+1) for x in testing_df['Age_under24_mean16']]\n",
        "testing_df['WindSpeedM(m/s)_mean16_log'] = [np.log1p(x+1) for x in testing_df['WindSpeedM(m/s)_mean16']]\n",
        "testing_df['Metro_station_mean16_log'] = [np.log1p(x+1) for x in testing_df['Metro_station_mean16']]\n",
        "testing_df['18Metrotrip_IN_mean16_log'] = [np.log1p(x+1) for x in testing_df['18Metrotrip_IN_mean16']]\n",
        "testing_df['18Metrotrip_OUT_mean16_log'] = [np.log1p(x+1) for x in testing_df['18Metrotrip_OUT_mean16']]\n",
        "testing_df['Metrotrip_IN_mean16_log'] = [np.log1p(x+1) for x in testing_df['Metrotrip_IN_mean16']]\n",
        "testing_df['Metrotrip_OUT_mean16_log'] = [np.log1p(x+1) for x in testing_df['Metrotrip_OUT_mean16']]\n",
        "testing_df['Bus_station_mean16_log'] = [np.log1p(x+1) for x in testing_df['Bus_station_mean16']]\n",
        "testing_df['Bike_station_mean16_log'] = [np.log1p(x+1) for x in testing_df['Bike_station_mean16']]\n",
        "testing_df['Bike_stand_mean16_log'] = [np.log1p(x+1) for x in testing_df['Bike_stand_mean16']]\n",
        "testing_df['BikeRoadCount(개)_mean16_log'] = [np.log1p(x+1) for x in testing_df['BikeRoadCount(개)_mean16']]\n",
        "testing_df['BikeRoadDistance(km)_mean16_log'] = [np.log1p(x+1) for x in testing_df['BikeRoadDistance(km)_mean16']]\n",
        "testing_df['Trip_Count_mean16_log'] = [np.log1p(x+1) for x in testing_df['Trip_Count_mean16']]\n",
        "testing_df['WindSpeedM(m/s)_std16_log'] = [np.log1p(x+1) for x in testing_df['WindSpeedM(m/s)_std16']]\n",
        "testing_df['18Metrotrip_IN_std16_log'] = [np.log1p(x+1) for x in testing_df['18Metrotrip_IN_std16']]\n",
        "testing_df['18Metrotrip_OUT_std16_log'] = [np.log1p(x+1) for x in testing_df['18Metrotrip_OUT_std16']]\n",
        "testing_df['Metrotrip_IN_std16_log'] = [np.log1p(x+1) for x in testing_df['Metrotrip_IN_std16']]\n",
        "testing_df['Metrotrip_OUT_std16_log'] = [np.log1p(x+1) for x in testing_df['Metrotrip_OUT_std16']]\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDHL5V38fHRJ"
      },
      "source": [
        "# applying differencing to remove trend from Trip_Count\n",
        "bike_df_check = bike_df[['Trip_Count']]\n",
        "bike_df_check['Trip_Count_diff'] = bike_df_check['Trip_Count'] - bike_df_check['Trip_Count'].shift()\n",
        "bike_df_check = bike_df_check.iloc[1:,]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tFZwT31sfHRK"
      },
      "source": [
        "# kpss test for Trip_Count\n",
        "kpss_test(testing_df, ['Trip_Count_mean16_log'])\n",
        "kpss_test(bike_df_temp, ['Trip_Count_t-1_log'])\n",
        "\n",
        "# adf test for Trip_Count\n",
        "adf_test(testing_df, ['Trip_Count_mean16_log'])\n",
        "adf_test(bike_df_temp, ['Trip_Count_t-1_log'])\n",
        "adf_test(bike_df_check, ['Trip_Count_diff'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAwG8fRYfHSA"
      },
      "source": [
        "# should keep this before the cleaning function has been created\n",
        "bike_df.drop(columns=['COVID/100000', 'weekday'], axis=1, inplace=True)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbDOIrUFfHSC"
      },
      "source": [
        "# drop the timestamp variable\n",
        "bike_df.drop(columns=['date_datetime'], inplace=True)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfHSsglAfHSD"
      },
      "source": [
        "# specify the window for rolling values\n",
        "window = 8"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ygd0jIvfHSG"
      },
      "source": [
        "# creating rolling values\n",
        "new_feat = ['Trip_Count', 'ENTROPYBIG',\t'ENTROPYSMALL',\t'110_RESIDENTIAL',\t'111_HOUSE',\t'112_APT',\t'120_INDUSTRIAL',\t'121_INDUSTRIAL',\t'130_COMMERCIAL',\t'131_COMMERCIAL_WORK',\t'132_MIXEDUSE',\t'140_RECREATIONAL',\t'141_RECREATIONAL',\t'150_TRANSIT',\t'160_PUBLIC',\t'GREENINFRA',\t'POPDensity(p/㎢)',\t'POPGender(Male/Female)',\t'Age_under24',\t'TempM(°C)',\t'WindSpeedM(m/s)',\t'RainM(mm)',\t'PM10',\t'PM2.5',\t'COVID',\t'Metro_station',\t'18Metrotrip_IN',\t'18Metrotrip_OUT',\t'Metrotrip_IN',\t'Metrotrip_OUT',\t'Bus_station',\t'BusINCount',\t'BusOUTCount',\t'Bike_station',\t'Bike_stand',\t'BikeRoadCount(개)',\t'BikeRoadDistance(km)']\n",
        "\n",
        "temp_df = pd.DataFrame()\n",
        "\n",
        "for col in new_feat:\n",
        "    col_mean = bike_df[col].rolling(window).mean()[(window-1):-1]\n",
        "    col_std = bike_df[col].rolling(window).std()[(window-1):-1]\n",
        "    temp_df[col+'_mean'+str(window)] = col_mean.values\n",
        "    temp_df[col+'_std'+str(window)] = col_std.values"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MY-slaWAfHSN"
      },
      "source": [
        "# remember to remove the first row from 16 rows from Trip_Count (target label)\n",
        "new_bike_df = bike_df.iloc[window:,:]\n",
        "bike_df_cat = bike_df_cat.iloc[window:,:]\n",
        "new_bike_df.reset_index(drop=True, inplace=True)\n",
        "bike_df_cat.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2f4zB_87fHTA"
      },
      "source": [
        "# merging both dataframes with features\n",
        "final_bike_df = new_bike_df.join(temp_df, how='left')\n",
        "final_bike_df = final_bike_df.merge(bike_df_cat, how='left')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88XKtLV-fHTD",
        "outputId": "d4fc9376-6ece-4ad6-fafa-9d8340d3c27c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "# assigning X and y\n",
        "y = final_bike_df['Trip_Count']\n",
        "X = final_bike_df.drop(columns=['Trip_Count'])\n",
        "\n",
        "final_bike_df['Date'] = pd.to_datetime(final_bike_df['Date'], format='%Y-%m-%d').astype(int)\n",
        "final_bike_df['Date'] = final_bike_df['Date'].apply(lambda x: str(x))\n",
        "final_bike_df['date_datetime'] = pd.to_datetime(final_bike_df['date_datetime'], format='%Y-%m-%d').astype(int)\n",
        "final_bike_df['date_datetime'] = final_bike_df['date_datetime'].apply(lambda x: str(x))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>ENTROPYBIG</th>\n",
              "      <th>ENTROPYSMALL</th>\n",
              "      <th>110_RESIDENTIAL</th>\n",
              "      <th>111_HOUSE</th>\n",
              "      <th>112_APT</th>\n",
              "      <th>120_INDUSTRIAL</th>\n",
              "      <th>121_INDUSTRIAL</th>\n",
              "      <th>130_COMMERCIAL</th>\n",
              "      <th>131_COMMERCIAL_WORK</th>\n",
              "      <th>132_MIXEDUSE</th>\n",
              "      <th>140_RECREATIONAL</th>\n",
              "      <th>141_RECREATIONAL</th>\n",
              "      <th>150_TRANSIT</th>\n",
              "      <th>160_PUBLIC</th>\n",
              "      <th>GREENINFRA</th>\n",
              "      <th>POPDensity(p/㎢)</th>\n",
              "      <th>POPGender(Male/Female)</th>\n",
              "      <th>Age_under24</th>\n",
              "      <th>TempM(°C)</th>\n",
              "      <th>WindSpeedM(m/s)</th>\n",
              "      <th>RainM(mm)</th>\n",
              "      <th>PM10</th>\n",
              "      <th>PM2.5</th>\n",
              "      <th>COVID</th>\n",
              "      <th>Metro_station</th>\n",
              "      <th>18Metrotrip_IN</th>\n",
              "      <th>18Metrotrip_OUT</th>\n",
              "      <th>Metrotrip_IN</th>\n",
              "      <th>Metrotrip_OUT</th>\n",
              "      <th>Bus_station</th>\n",
              "      <th>BusINCount</th>\n",
              "      <th>BusOUTCount</th>\n",
              "      <th>Bike_station</th>\n",
              "      <th>Bike_stand</th>\n",
              "      <th>BikeRoadCount(개)</th>\n",
              "      <th>BikeRoadDistance(km)</th>\n",
              "      <th>SKTPOP</th>\n",
              "      <th>holiday</th>\n",
              "      <th>season_spring</th>\n",
              "      <th>season_summer</th>\n",
              "      <th>season_fall</th>\n",
              "      <th>season_winter</th>\n",
              "      <th>weekday_1</th>\n",
              "      <th>weekday_2</th>\n",
              "      <th>weekday_3</th>\n",
              "      <th>weekday_4</th>\n",
              "      <th>weekday_5</th>\n",
              "      <th>weekday_6</th>\n",
              "      <th>working_day</th>\n",
              "      <th>Trip_Count_t-1</th>\n",
              "      <th>Trip_Count_mean8</th>\n",
              "      <th>Trip_Count_std8</th>\n",
              "      <th>ENTROPYBIG_mean8</th>\n",
              "      <th>ENTROPYBIG_std8</th>\n",
              "      <th>ENTROPYSMALL_mean8</th>\n",
              "      <th>ENTROPYSMALL_std8</th>\n",
              "      <th>110_RESIDENTIAL_mean8</th>\n",
              "      <th>110_RESIDENTIAL_std8</th>\n",
              "      <th>111_HOUSE_mean8</th>\n",
              "      <th>111_HOUSE_std8</th>\n",
              "      <th>112_APT_mean8</th>\n",
              "      <th>112_APT_std8</th>\n",
              "      <th>120_INDUSTRIAL_mean8</th>\n",
              "      <th>120_INDUSTRIAL_std8</th>\n",
              "      <th>121_INDUSTRIAL_mean8</th>\n",
              "      <th>121_INDUSTRIAL_std8</th>\n",
              "      <th>130_COMMERCIAL_mean8</th>\n",
              "      <th>130_COMMERCIAL_std8</th>\n",
              "      <th>131_COMMERCIAL_WORK_mean8</th>\n",
              "      <th>131_COMMERCIAL_WORK_std8</th>\n",
              "      <th>132_MIXEDUSE_mean8</th>\n",
              "      <th>132_MIXEDUSE_std8</th>\n",
              "      <th>140_RECREATIONAL_mean8</th>\n",
              "      <th>140_RECREATIONAL_std8</th>\n",
              "      <th>141_RECREATIONAL_mean8</th>\n",
              "      <th>141_RECREATIONAL_std8</th>\n",
              "      <th>150_TRANSIT_mean8</th>\n",
              "      <th>150_TRANSIT_std8</th>\n",
              "      <th>160_PUBLIC_mean8</th>\n",
              "      <th>160_PUBLIC_std8</th>\n",
              "      <th>GREENINFRA_mean8</th>\n",
              "      <th>GREENINFRA_std8</th>\n",
              "      <th>POPDensity(p/㎢)_mean8</th>\n",
              "      <th>POPDensity(p/㎢)_std8</th>\n",
              "      <th>POPGender(Male/Female)_mean8</th>\n",
              "      <th>POPGender(Male/Female)_std8</th>\n",
              "      <th>Age_under24_mean8</th>\n",
              "      <th>Age_under24_std8</th>\n",
              "      <th>TempM(°C)_mean8</th>\n",
              "      <th>TempM(°C)_std8</th>\n",
              "      <th>WindSpeedM(m/s)_mean8</th>\n",
              "      <th>WindSpeedM(m/s)_std8</th>\n",
              "      <th>RainM(mm)_mean8</th>\n",
              "      <th>RainM(mm)_std8</th>\n",
              "      <th>PM10_mean8</th>\n",
              "      <th>PM10_std8</th>\n",
              "      <th>PM2.5_mean8</th>\n",
              "      <th>PM2.5_std8</th>\n",
              "      <th>COVID_mean8</th>\n",
              "      <th>COVID_std8</th>\n",
              "      <th>Metro_station_mean8</th>\n",
              "      <th>Metro_station_std8</th>\n",
              "      <th>18Metrotrip_IN_mean8</th>\n",
              "      <th>18Metrotrip_IN_std8</th>\n",
              "      <th>18Metrotrip_OUT_mean8</th>\n",
              "      <th>18Metrotrip_OUT_std8</th>\n",
              "      <th>Metrotrip_IN_mean8</th>\n",
              "      <th>Metrotrip_IN_std8</th>\n",
              "      <th>Metrotrip_OUT_mean8</th>\n",
              "      <th>Metrotrip_OUT_std8</th>\n",
              "      <th>Bus_station_mean8</th>\n",
              "      <th>Bus_station_std8</th>\n",
              "      <th>BusINCount_mean8</th>\n",
              "      <th>BusINCount_std8</th>\n",
              "      <th>BusOUTCount_mean8</th>\n",
              "      <th>BusOUTCount_std8</th>\n",
              "      <th>Bike_station_mean8</th>\n",
              "      <th>Bike_station_std8</th>\n",
              "      <th>Bike_stand_mean8</th>\n",
              "      <th>Bike_stand_std8</th>\n",
              "      <th>BikeRoadCount(개)_mean8</th>\n",
              "      <th>BikeRoadCount(개)_std8</th>\n",
              "      <th>BikeRoadDistance(km)_mean8</th>\n",
              "      <th>BikeRoadDistance(km)_std8</th>\n",
              "      <th>COVID/100000</th>\n",
              "      <th>date_datetime</th>\n",
              "      <th>weekday</th>\n",
              "      <th>PM10_good</th>\n",
              "      <th>PM10_moderate</th>\n",
              "      <th>PM10_unhealthy</th>\n",
              "      <th>PM10_very_unhealthy</th>\n",
              "      <th>PM25_good</th>\n",
              "      <th>PM25_moderate</th>\n",
              "      <th>PM25_unhealthy</th>\n",
              "      <th>PM25_very_unhealthy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1552089600000000000</td>\n",
              "      <td>0.568</td>\n",
              "      <td>0.658</td>\n",
              "      <td>26.798993</td>\n",
              "      <td>5.660696</td>\n",
              "      <td>5.374966</td>\n",
              "      <td>0.110598</td>\n",
              "      <td>0.144978</td>\n",
              "      <td>4.704451</td>\n",
              "      <td>4.557102</td>\n",
              "      <td>0.132349</td>\n",
              "      <td>0.68001</td>\n",
              "      <td>0.845942</td>\n",
              "      <td>9.150575</td>\n",
              "      <td>3.376849</td>\n",
              "      <td>48.116125</td>\n",
              "      <td>16245</td>\n",
              "      <td>94.36</td>\n",
              "      <td>0.218147</td>\n",
              "      <td>5.645833</td>\n",
              "      <td>0.837500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>65463</td>\n",
              "      <td>65411</td>\n",
              "      <td>90049</td>\n",
              "      <td>85338</td>\n",
              "      <td>390</td>\n",
              "      <td>123134</td>\n",
              "      <td>118254</td>\n",
              "      <td>33</td>\n",
              "      <td>396</td>\n",
              "      <td>4</td>\n",
              "      <td>12.3</td>\n",
              "      <td>5765180</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>465.0</td>\n",
              "      <td>395.625</td>\n",
              "      <td>55.538757</td>\n",
              "      <td>0.568</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.658</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.798993</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.660696</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.374966</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.110598</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.144978</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.704451</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.557102</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.132349</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.68001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.845942</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.150575</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.376849</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.116125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16245.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.218147</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.414062</td>\n",
              "      <td>1.344689</td>\n",
              "      <td>1.141146</td>\n",
              "      <td>0.193706</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>108.500</td>\n",
              "      <td>46.213789</td>\n",
              "      <td>76.875</td>\n",
              "      <td>36.290249</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71326.375</td>\n",
              "      <td>15551.801659</td>\n",
              "      <td>71975.500</td>\n",
              "      <td>15500.323951</td>\n",
              "      <td>95244.750</td>\n",
              "      <td>18566.365463</td>\n",
              "      <td>91519.000</td>\n",
              "      <td>17933.040767</td>\n",
              "      <td>390.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>136703.250</td>\n",
              "      <td>30918.805871</td>\n",
              "      <td>132333.50</td>\n",
              "      <td>29409.877481</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>396.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1552176000000000000</td>\n",
              "      <td>0.568</td>\n",
              "      <td>0.658</td>\n",
              "      <td>26.798993</td>\n",
              "      <td>5.660696</td>\n",
              "      <td>5.374966</td>\n",
              "      <td>0.110598</td>\n",
              "      <td>0.144978</td>\n",
              "      <td>4.704451</td>\n",
              "      <td>4.557102</td>\n",
              "      <td>0.132349</td>\n",
              "      <td>0.68001</td>\n",
              "      <td>0.845942</td>\n",
              "      <td>9.150575</td>\n",
              "      <td>3.376849</td>\n",
              "      <td>48.116125</td>\n",
              "      <td>16245</td>\n",
              "      <td>94.36</td>\n",
              "      <td>0.218147</td>\n",
              "      <td>9.050000</td>\n",
              "      <td>1.120833</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>53.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>47242</td>\n",
              "      <td>48252</td>\n",
              "      <td>66210</td>\n",
              "      <td>63574</td>\n",
              "      <td>390</td>\n",
              "      <td>90996</td>\n",
              "      <td>88587</td>\n",
              "      <td>33</td>\n",
              "      <td>396</td>\n",
              "      <td>4</td>\n",
              "      <td>12.3</td>\n",
              "      <td>6066380</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>525.0</td>\n",
              "      <td>419.500</td>\n",
              "      <td>65.434809</td>\n",
              "      <td>0.568</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.658</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.798993</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.660696</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.374966</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.110598</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.144978</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.704451</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.557102</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.132349</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.68001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.845942</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.150575</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.376849</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.116125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16245.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.218147</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.356771</td>\n",
              "      <td>1.416327</td>\n",
              "      <td>1.088542</td>\n",
              "      <td>0.213469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>100.375</td>\n",
              "      <td>52.768869</td>\n",
              "      <td>71.250</td>\n",
              "      <td>40.566524</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72530.125</td>\n",
              "      <td>14519.701536</td>\n",
              "      <td>73210.500</td>\n",
              "      <td>14354.141155</td>\n",
              "      <td>96948.000</td>\n",
              "      <td>17165.158008</td>\n",
              "      <td>93143.250</td>\n",
              "      <td>16477.587763</td>\n",
              "      <td>390.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>139357.250</td>\n",
              "      <td>28305.617089</td>\n",
              "      <td>134820.75</td>\n",
              "      <td>26856.381970</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>396.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2019-03-10</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1552262400000000000</td>\n",
              "      <td>0.568</td>\n",
              "      <td>0.658</td>\n",
              "      <td>26.798993</td>\n",
              "      <td>5.660696</td>\n",
              "      <td>5.374966</td>\n",
              "      <td>0.110598</td>\n",
              "      <td>0.144978</td>\n",
              "      <td>4.704451</td>\n",
              "      <td>4.557102</td>\n",
              "      <td>0.132349</td>\n",
              "      <td>0.68001</td>\n",
              "      <td>0.845942</td>\n",
              "      <td>9.150575</td>\n",
              "      <td>3.376849</td>\n",
              "      <td>48.116125</td>\n",
              "      <td>16245</td>\n",
              "      <td>94.36</td>\n",
              "      <td>0.218147</td>\n",
              "      <td>6.145833</td>\n",
              "      <td>1.191667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>72.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>81911</td>\n",
              "      <td>82425</td>\n",
              "      <td>108085</td>\n",
              "      <td>103965</td>\n",
              "      <td>390</td>\n",
              "      <td>155670</td>\n",
              "      <td>150210</td>\n",
              "      <td>33</td>\n",
              "      <td>396</td>\n",
              "      <td>4</td>\n",
              "      <td>12.3</td>\n",
              "      <td>5455400</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>479.0</td>\n",
              "      <td>432.875</td>\n",
              "      <td>65.274010</td>\n",
              "      <td>0.568</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.658</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.798993</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.660696</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.374966</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.110598</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.144978</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.704451</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.557102</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.132349</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.68001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.845942</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.150575</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.376849</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.116125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16245.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.218147</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.663542</td>\n",
              "      <td>1.491739</td>\n",
              "      <td>1.130208</td>\n",
              "      <td>0.175464</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>93.375</td>\n",
              "      <td>55.123077</td>\n",
              "      <td>66.375</td>\n",
              "      <td>42.355426</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71359.250</td>\n",
              "      <td>16260.344345</td>\n",
              "      <td>72037.875</td>\n",
              "      <td>16087.072889</td>\n",
              "      <td>95465.000</td>\n",
              "      <td>19396.247804</td>\n",
              "      <td>91712.500</td>\n",
              "      <td>18632.108538</td>\n",
              "      <td>390.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>136854.625</td>\n",
              "      <td>31834.373578</td>\n",
              "      <td>132472.00</td>\n",
              "      <td>30211.454734</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>396.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2019-03-11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1552348800000000000</td>\n",
              "      <td>0.568</td>\n",
              "      <td>0.658</td>\n",
              "      <td>26.798993</td>\n",
              "      <td>5.660696</td>\n",
              "      <td>5.374966</td>\n",
              "      <td>0.110598</td>\n",
              "      <td>0.144978</td>\n",
              "      <td>4.704451</td>\n",
              "      <td>4.557102</td>\n",
              "      <td>0.132349</td>\n",
              "      <td>0.68001</td>\n",
              "      <td>0.845942</td>\n",
              "      <td>9.150575</td>\n",
              "      <td>3.376849</td>\n",
              "      <td>48.116125</td>\n",
              "      <td>16245</td>\n",
              "      <td>94.36</td>\n",
              "      <td>0.218147</td>\n",
              "      <td>4.462500</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.020833</td>\n",
              "      <td>80.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>81225</td>\n",
              "      <td>82658</td>\n",
              "      <td>106328</td>\n",
              "      <td>103187</td>\n",
              "      <td>390</td>\n",
              "      <td>155042</td>\n",
              "      <td>149861</td>\n",
              "      <td>33</td>\n",
              "      <td>396</td>\n",
              "      <td>4</td>\n",
              "      <td>12.3</td>\n",
              "      <td>5417900</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>450.0</td>\n",
              "      <td>437.125</td>\n",
              "      <td>65.125017</td>\n",
              "      <td>0.568</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.658</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.798993</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.660696</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.374966</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.110598</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.144978</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.704451</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.557102</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.132349</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.68001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.845942</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.150575</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.376849</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.116125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16245.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.218147</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.396875</td>\n",
              "      <td>1.555293</td>\n",
              "      <td>1.151563</td>\n",
              "      <td>0.170578</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>89.375</td>\n",
              "      <td>55.402263</td>\n",
              "      <td>63.250</td>\n",
              "      <td>42.717511</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>75772.750</td>\n",
              "      <td>13057.166853</td>\n",
              "      <td>76368.500</td>\n",
              "      <td>12988.845555</td>\n",
              "      <td>100745.750</td>\n",
              "      <td>15547.048234</td>\n",
              "      <td>96745.375</td>\n",
              "      <td>15085.401078</td>\n",
              "      <td>390.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>145355.500</td>\n",
              "      <td>25213.586893</td>\n",
              "      <td>140409.00</td>\n",
              "      <td>24219.231756</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>396.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2019-03-12</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1552435200000000000</td>\n",
              "      <td>0.568</td>\n",
              "      <td>0.658</td>\n",
              "      <td>26.798993</td>\n",
              "      <td>5.660696</td>\n",
              "      <td>5.374966</td>\n",
              "      <td>0.110598</td>\n",
              "      <td>0.144978</td>\n",
              "      <td>4.704451</td>\n",
              "      <td>4.557102</td>\n",
              "      <td>0.132349</td>\n",
              "      <td>0.68001</td>\n",
              "      <td>0.845942</td>\n",
              "      <td>9.150575</td>\n",
              "      <td>3.376849</td>\n",
              "      <td>48.116125</td>\n",
              "      <td>16245</td>\n",
              "      <td>94.36</td>\n",
              "      <td>0.218147</td>\n",
              "      <td>3.600000</td>\n",
              "      <td>2.158333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>40.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>82434</td>\n",
              "      <td>83425</td>\n",
              "      <td>108827</td>\n",
              "      <td>105100</td>\n",
              "      <td>390</td>\n",
              "      <td>156493</td>\n",
              "      <td>151236</td>\n",
              "      <td>33</td>\n",
              "      <td>396</td>\n",
              "      <td>4</td>\n",
              "      <td>12.3</td>\n",
              "      <td>5416560</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>379.0</td>\n",
              "      <td>431.750</td>\n",
              "      <td>68.251112</td>\n",
              "      <td>0.568</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.658</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.798993</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.660696</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.374966</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.110598</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.144978</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.704451</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.557102</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.132349</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.68001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.845942</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.150575</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.376849</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.116125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16245.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.218147</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.743229</td>\n",
              "      <td>1.551912</td>\n",
              "      <td>1.254167</td>\n",
              "      <td>0.346109</td>\n",
              "      <td>0.002604</td>\n",
              "      <td>0.007366</td>\n",
              "      <td>80.375</td>\n",
              "      <td>49.286154</td>\n",
              "      <td>56.000</td>\n",
              "      <td>39.413558</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>75875.250</td>\n",
              "      <td>13101.867280</td>\n",
              "      <td>76561.750</td>\n",
              "      <td>13080.678311</td>\n",
              "      <td>100778.625</td>\n",
              "      <td>15560.175402</td>\n",
              "      <td>96888.250</td>\n",
              "      <td>15148.036551</td>\n",
              "      <td>390.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>145007.625</td>\n",
              "      <td>25035.536069</td>\n",
              "      <td>140042.75</td>\n",
              "      <td>24026.626823</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>396.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2019-03-13</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Date  ENTROPYBIG  ENTROPYSMALL  110_RESIDENTIAL  111_HOUSE  \\\n",
              "0  1552089600000000000       0.568         0.658        26.798993   5.660696   \n",
              "1  1552176000000000000       0.568         0.658        26.798993   5.660696   \n",
              "2  1552262400000000000       0.568         0.658        26.798993   5.660696   \n",
              "3  1552348800000000000       0.568         0.658        26.798993   5.660696   \n",
              "4  1552435200000000000       0.568         0.658        26.798993   5.660696   \n",
              "\n",
              "    112_APT  120_INDUSTRIAL  121_INDUSTRIAL  130_COMMERCIAL  \\\n",
              "0  5.374966        0.110598        0.144978        4.704451   \n",
              "1  5.374966        0.110598        0.144978        4.704451   \n",
              "2  5.374966        0.110598        0.144978        4.704451   \n",
              "3  5.374966        0.110598        0.144978        4.704451   \n",
              "4  5.374966        0.110598        0.144978        4.704451   \n",
              "\n",
              "   131_COMMERCIAL_WORK  132_MIXEDUSE  140_RECREATIONAL  141_RECREATIONAL  \\\n",
              "0             4.557102      0.132349           0.68001          0.845942   \n",
              "1             4.557102      0.132349           0.68001          0.845942   \n",
              "2             4.557102      0.132349           0.68001          0.845942   \n",
              "3             4.557102      0.132349           0.68001          0.845942   \n",
              "4             4.557102      0.132349           0.68001          0.845942   \n",
              "\n",
              "   150_TRANSIT  160_PUBLIC  GREENINFRA  POPDensity(p/㎢)  \\\n",
              "0     9.150575    3.376849   48.116125            16245   \n",
              "1     9.150575    3.376849   48.116125            16245   \n",
              "2     9.150575    3.376849   48.116125            16245   \n",
              "3     9.150575    3.376849   48.116125            16245   \n",
              "4     9.150575    3.376849   48.116125            16245   \n",
              "\n",
              "   POPGender(Male/Female)  Age_under24  TempM(°C)  WindSpeedM(m/s)  RainM(mm)  \\\n",
              "0                   94.36     0.218147   5.645833         0.837500   0.000000   \n",
              "1                   94.36     0.218147   9.050000         1.120833   0.000000   \n",
              "2                   94.36     0.218147   6.145833         1.191667   0.000000   \n",
              "3                   94.36     0.218147   4.462500         2.000000   0.020833   \n",
              "4                   94.36     0.218147   3.600000         2.158333   0.000000   \n",
              "\n",
              "   PM10  PM2.5  COVID  Metro_station  18Metrotrip_IN  18Metrotrip_OUT  \\\n",
              "0  37.0   26.0      0              8           65463            65411   \n",
              "1  53.0   36.0      0              8           47242            48252   \n",
              "2  72.0   48.0      0              8           81911            82425   \n",
              "3  80.0   47.0      0              8           81225            82658   \n",
              "4  40.0   10.0      0              8           82434            83425   \n",
              "\n",
              "   Metrotrip_IN  Metrotrip_OUT  Bus_station  BusINCount  BusOUTCount  \\\n",
              "0         90049          85338          390      123134       118254   \n",
              "1         66210          63574          390       90996        88587   \n",
              "2        108085         103965          390      155670       150210   \n",
              "3        106328         103187          390      155042       149861   \n",
              "4        108827         105100          390      156493       151236   \n",
              "\n",
              "   Bike_station  Bike_stand  BikeRoadCount(개)  BikeRoadDistance(km)   SKTPOP  \\\n",
              "0            33         396                 4                  12.3  5765180   \n",
              "1            33         396                 4                  12.3  6066380   \n",
              "2            33         396                 4                  12.3  5455400   \n",
              "3            33         396                 4                  12.3  5417900   \n",
              "4            33         396                 4                  12.3  5416560   \n",
              "\n",
              "   holiday  season_spring  season_summer  season_fall  season_winter  \\\n",
              "0        0              1              0            0              0   \n",
              "1        0              1              0            0              0   \n",
              "2        0              1              0            0              0   \n",
              "3        0              1              0            0              0   \n",
              "4        0              1              0            0              0   \n",
              "\n",
              "   weekday_1  weekday_2  weekday_3  weekday_4  weekday_5  weekday_6  \\\n",
              "0          0          0          0          0          1          0   \n",
              "1          0          0          0          0          0          1   \n",
              "2          0          0          0          0          0          0   \n",
              "3          1          0          0          0          0          0   \n",
              "4          0          1          0          0          0          0   \n",
              "\n",
              "   working_day  Trip_Count_t-1  Trip_Count_mean8  Trip_Count_std8  \\\n",
              "0            1           465.0           395.625        55.538757   \n",
              "1            0           525.0           419.500        65.434809   \n",
              "2            0           479.0           432.875        65.274010   \n",
              "3            1           450.0           437.125        65.125017   \n",
              "4            1           379.0           431.750        68.251112   \n",
              "\n",
              "   ENTROPYBIG_mean8  ENTROPYBIG_std8  ENTROPYSMALL_mean8  ENTROPYSMALL_std8  \\\n",
              "0             0.568              0.0               0.658                0.0   \n",
              "1             0.568              0.0               0.658                0.0   \n",
              "2             0.568              0.0               0.658                0.0   \n",
              "3             0.568              0.0               0.658                0.0   \n",
              "4             0.568              0.0               0.658                0.0   \n",
              "\n",
              "   110_RESIDENTIAL_mean8  110_RESIDENTIAL_std8  111_HOUSE_mean8  \\\n",
              "0              26.798993                   0.0         5.660696   \n",
              "1              26.798993                   0.0         5.660696   \n",
              "2              26.798993                   0.0         5.660696   \n",
              "3              26.798993                   0.0         5.660696   \n",
              "4              26.798993                   0.0         5.660696   \n",
              "\n",
              "   111_HOUSE_std8  112_APT_mean8  112_APT_std8  120_INDUSTRIAL_mean8  \\\n",
              "0             0.0       5.374966           0.0              0.110598   \n",
              "1             0.0       5.374966           0.0              0.110598   \n",
              "2             0.0       5.374966           0.0              0.110598   \n",
              "3             0.0       5.374966           0.0              0.110598   \n",
              "4             0.0       5.374966           0.0              0.110598   \n",
              "\n",
              "   120_INDUSTRIAL_std8  121_INDUSTRIAL_mean8  121_INDUSTRIAL_std8  \\\n",
              "0                  0.0              0.144978                  0.0   \n",
              "1                  0.0              0.144978                  0.0   \n",
              "2                  0.0              0.144978                  0.0   \n",
              "3                  0.0              0.144978                  0.0   \n",
              "4                  0.0              0.144978                  0.0   \n",
              "\n",
              "   130_COMMERCIAL_mean8  130_COMMERCIAL_std8  131_COMMERCIAL_WORK_mean8  \\\n",
              "0              4.704451                  0.0                   4.557102   \n",
              "1              4.704451                  0.0                   4.557102   \n",
              "2              4.704451                  0.0                   4.557102   \n",
              "3              4.704451                  0.0                   4.557102   \n",
              "4              4.704451                  0.0                   4.557102   \n",
              "\n",
              "   131_COMMERCIAL_WORK_std8  132_MIXEDUSE_mean8  132_MIXEDUSE_std8  \\\n",
              "0                       0.0            0.132349                0.0   \n",
              "1                       0.0            0.132349                0.0   \n",
              "2                       0.0            0.132349                0.0   \n",
              "3                       0.0            0.132349                0.0   \n",
              "4                       0.0            0.132349                0.0   \n",
              "\n",
              "   140_RECREATIONAL_mean8  140_RECREATIONAL_std8  141_RECREATIONAL_mean8  \\\n",
              "0                 0.68001                    0.0                0.845942   \n",
              "1                 0.68001                    0.0                0.845942   \n",
              "2                 0.68001                    0.0                0.845942   \n",
              "3                 0.68001                    0.0                0.845942   \n",
              "4                 0.68001                    0.0                0.845942   \n",
              "\n",
              "   141_RECREATIONAL_std8  150_TRANSIT_mean8  150_TRANSIT_std8  \\\n",
              "0                    0.0           9.150575               0.0   \n",
              "1                    0.0           9.150575               0.0   \n",
              "2                    0.0           9.150575               0.0   \n",
              "3                    0.0           9.150575               0.0   \n",
              "4                    0.0           9.150575               0.0   \n",
              "\n",
              "   160_PUBLIC_mean8  160_PUBLIC_std8  GREENINFRA_mean8  GREENINFRA_std8  \\\n",
              "0          3.376849              0.0         48.116125              0.0   \n",
              "1          3.376849              0.0         48.116125              0.0   \n",
              "2          3.376849              0.0         48.116125              0.0   \n",
              "3          3.376849              0.0         48.116125              0.0   \n",
              "4          3.376849              0.0         48.116125              0.0   \n",
              "\n",
              "   POPDensity(p/㎢)_mean8  POPDensity(p/㎢)_std8  POPGender(Male/Female)_mean8  \\\n",
              "0                16245.0                   0.0                         94.36   \n",
              "1                16245.0                   0.0                         94.36   \n",
              "2                16245.0                   0.0                         94.36   \n",
              "3                16245.0                   0.0                         94.36   \n",
              "4                16245.0                   0.0                         94.36   \n",
              "\n",
              "   POPGender(Male/Female)_std8  Age_under24_mean8  Age_under24_std8  \\\n",
              "0                          0.0           0.218147               0.0   \n",
              "1                          0.0           0.218147               0.0   \n",
              "2                          0.0           0.218147               0.0   \n",
              "3                          0.0           0.218147               0.0   \n",
              "4                          0.0           0.218147               0.0   \n",
              "\n",
              "   TempM(°C)_mean8  TempM(°C)_std8  WindSpeedM(m/s)_mean8  \\\n",
              "0         7.414062        1.344689               1.141146   \n",
              "1         7.356771        1.416327               1.088542   \n",
              "2         7.663542        1.491739               1.130208   \n",
              "3         7.396875        1.555293               1.151563   \n",
              "4         6.743229        1.551912               1.254167   \n",
              "\n",
              "   WindSpeedM(m/s)_std8  RainM(mm)_mean8  RainM(mm)_std8  PM10_mean8  \\\n",
              "0              0.193706         0.000000        0.000000     108.500   \n",
              "1              0.213469         0.000000        0.000000     100.375   \n",
              "2              0.175464         0.000000        0.000000      93.375   \n",
              "3              0.170578         0.000000        0.000000      89.375   \n",
              "4              0.346109         0.002604        0.007366      80.375   \n",
              "\n",
              "   PM10_std8  PM2.5_mean8  PM2.5_std8  COVID_mean8  COVID_std8  \\\n",
              "0  46.213789       76.875   36.290249          0.0         0.0   \n",
              "1  52.768869       71.250   40.566524          0.0         0.0   \n",
              "2  55.123077       66.375   42.355426          0.0         0.0   \n",
              "3  55.402263       63.250   42.717511          0.0         0.0   \n",
              "4  49.286154       56.000   39.413558          0.0         0.0   \n",
              "\n",
              "   Metro_station_mean8  Metro_station_std8  18Metrotrip_IN_mean8  \\\n",
              "0                  8.0                 0.0             71326.375   \n",
              "1                  8.0                 0.0             72530.125   \n",
              "2                  8.0                 0.0             71359.250   \n",
              "3                  8.0                 0.0             75772.750   \n",
              "4                  8.0                 0.0             75875.250   \n",
              "\n",
              "   18Metrotrip_IN_std8  18Metrotrip_OUT_mean8  18Metrotrip_OUT_std8  \\\n",
              "0         15551.801659              71975.500          15500.323951   \n",
              "1         14519.701536              73210.500          14354.141155   \n",
              "2         16260.344345              72037.875          16087.072889   \n",
              "3         13057.166853              76368.500          12988.845555   \n",
              "4         13101.867280              76561.750          13080.678311   \n",
              "\n",
              "   Metrotrip_IN_mean8  Metrotrip_IN_std8  Metrotrip_OUT_mean8  \\\n",
              "0           95244.750       18566.365463            91519.000   \n",
              "1           96948.000       17165.158008            93143.250   \n",
              "2           95465.000       19396.247804            91712.500   \n",
              "3          100745.750       15547.048234            96745.375   \n",
              "4          100778.625       15560.175402            96888.250   \n",
              "\n",
              "   Metrotrip_OUT_std8  Bus_station_mean8  Bus_station_std8  BusINCount_mean8  \\\n",
              "0        17933.040767              390.0               0.0        136703.250   \n",
              "1        16477.587763              390.0               0.0        139357.250   \n",
              "2        18632.108538              390.0               0.0        136854.625   \n",
              "3        15085.401078              390.0               0.0        145355.500   \n",
              "4        15148.036551              390.0               0.0        145007.625   \n",
              "\n",
              "   BusINCount_std8  BusOUTCount_mean8  BusOUTCount_std8  Bike_station_mean8  \\\n",
              "0     30918.805871          132333.50      29409.877481                33.0   \n",
              "1     28305.617089          134820.75      26856.381970                33.0   \n",
              "2     31834.373578          132472.00      30211.454734                33.0   \n",
              "3     25213.586893          140409.00      24219.231756                33.0   \n",
              "4     25035.536069          140042.75      24026.626823                33.0   \n",
              "\n",
              "   Bike_station_std8  Bike_stand_mean8  Bike_stand_std8  \\\n",
              "0                0.0             396.0              0.0   \n",
              "1                0.0             396.0              0.0   \n",
              "2                0.0             396.0              0.0   \n",
              "3                0.0             396.0              0.0   \n",
              "4                0.0             396.0              0.0   \n",
              "\n",
              "   BikeRoadCount(개)_mean8  BikeRoadCount(개)_std8  BikeRoadDistance(km)_mean8  \\\n",
              "0                     4.0                    0.0                        12.3   \n",
              "1                     4.0                    0.0                        12.3   \n",
              "2                     4.0                    0.0                        12.3   \n",
              "3                     4.0                    0.0                        12.3   \n",
              "4                     4.0                    0.0                        12.3   \n",
              "\n",
              "   BikeRoadDistance(km)_std8  COVID/100000 date_datetime  weekday  PM10_good  \\\n",
              "0                        0.0           NaN           NaT      NaN        NaN   \n",
              "1                        0.0           0.0    2019-03-10      6.0        0.0   \n",
              "2                        0.0           0.0    2019-03-11      0.0        0.0   \n",
              "3                        0.0           0.0    2019-03-12      1.0        0.0   \n",
              "4                        0.0           0.0    2019-03-13      2.0        0.0   \n",
              "\n",
              "   PM10_moderate  PM10_unhealthy  PM10_very_unhealthy  PM25_good  \\\n",
              "0            NaN             NaN                  NaN        NaN   \n",
              "1            1.0             0.0                  0.0        0.0   \n",
              "2            1.0             0.0                  0.0        0.0   \n",
              "3            1.0             0.0                  0.0        0.0   \n",
              "4            0.0             0.0                  0.0        0.0   \n",
              "\n",
              "   PM25_moderate  PM25_unhealthy  PM25_very_unhealthy  \n",
              "0            NaN             NaN                  NaN  \n",
              "1            1.0             0.0                  0.0  \n",
              "2            0.0             1.0                  0.0  \n",
              "3            0.0             1.0                  0.0  \n",
              "4            0.0             1.0                  0.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQGH7EZEfHTM"
      },
      "source": [
        "# creating a class that I can use in the ML pipeline that prints out the transformed data that will enter the model\n",
        "class Debug(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        self.shape = X.shape\n",
        "        print(self.shape)\n",
        "        X_df = pd.DataFrame(X)\n",
        "        print(X_df)\n",
        "        # print(X_df.to_string()) # can only be print like this without running LagVars() to avoid crashing\n",
        "        # what other output you want\n",
        "        return X"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3tz-TubfHTQ",
        "outputId": "d6ad60b9-6174-4d13-a3ce-0eb94fc3eae6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# assigning X and y for the univariate naive prediction\n",
        "y_naive = (final_bike_df['Trip_Count'].copy())\n",
        "X_naive = y_naive.copy()\n",
        "X_naive = pd.DataFrame(data=X_naive, columns=['Trip_Count'])\n",
        "\n",
        "# getting the start time\n",
        "start_time = datetime.now()\n",
        "\n",
        "# creating y_pred\n",
        "y_pred = (X_naive.shift())[1:]\n",
        "# adjusting length of the actual target values\n",
        "y_naive = y_naive[1:]\n",
        "\n",
        "# get final time\n",
        "end_time = datetime.now()\n",
        "print('Total running time of naive predictor:', (end_time - start_time).total_seconds())\n",
        "\n",
        "# calculating the scores for the last value method\n",
        "print('RMSE:', np.sqrt(mean_squared_error(y_naive, y_pred)))\n",
        "print('RMSLE:', np.sqrt(mean_squared_log_error(y_naive, y_pred)))\n",
        "print('MAE:', mean_absolute_error(y_naive, y_pred))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total running time of naive predictor: 0.000533\n",
            "RMSE: 793.5107478722404\n",
            "RMSLE: 0.4937500277399892\n",
            "MAE: 468.14723976704124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRV2zwHFfHTR"
      },
      "source": [
        "#### Preparation code for using differenced y for predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YKsPxcdfHTS",
        "outputId": "8c3a9b5f-4f12-48fd-a107-2e7f79718e6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_log = final_bike_df[['Trip_Count']].copy()\n",
        "y_log['Trip_Count'] = y_log['Trip_Count'].apply(lambda x: np.log1p(x+1))\n",
        "y_shift = y_log.shift(1)\n",
        "y_diff = (y_log - y_shift)[1:]\n",
        "X_diff = X[1:]\n",
        "\n",
        "y_diff.reset_index(drop=True, inplace=True)\n",
        "X_diff.reset_index(drop=True, inplace=True)\n",
        "\n",
        "X_diff['Date'] = pd.to_datetime(X_diff['Date'], format='%Y-%m-%d').astype(int)\n",
        "X_diff['Date'] = X_diff['Date'].apply(lambda x: str(x))\n",
        "X_diff['date_datetime'] = pd.to_datetime(X_diff['date_datetime'], format='%Y-%m-%d').astype(int)\n",
        "X_diff['date_datetime'] = X_diff['date_datetime'].apply(lambda x: str(x))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPv4LzjkfHTT"
      },
      "source": [
        "# initializing the model which is a Random Forest model and uses default hyperparameters\n",
        "model_rf_diff = RandomForestRegressor(random_state=42)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT-UyAOSfHTU",
        "outputId": "42461f03-1e77-4ea5-cf53-cb915125d0ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# creating and fitting the ML pipeline\n",
        "trend_features = ['Trip_Count_mean'+str(window), 'Trip_Count_std'+str(window), 'Trip_Count_t-1']\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n",
        "], remainder='passthrough')\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n",
        "    ('model', model_rf_diff)\n",
        "])\n",
        "\n",
        "pipeline_rf_diff = pipeline.fit(X_diff, y_diff)\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py:354: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  self._final_estimator.fit(Xt, y, **fit_params)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZto4mG3fHTd"
      },
      "source": [
        "# creating a timeseries split of the datasets\n",
        "time_split = TimeSeriesSplit(n_splits=10)\n",
        "\n",
        "# doing cross validation on the chunks of data and calculating scores\n",
        "scores_rf_diff = cross_validate(pipeline_rf_diff, X_diff, y_diff, cv=time_split,\n",
        "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error'],\n",
        "                         return_train_score=True, n_jobs=-1)\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MLiMwZjhfHTe",
        "outputId": "3138cc62-d37f-4a38-e2fe-440461736fac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# root mean squared error\n",
        "print('Average RMSE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_rf_diff['train_neg_mean_squared_error']])/len(scores_rf_diff['train_neg_mean_squared_error']))\n",
        "print('Average RMSE test data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_rf_diff['test_neg_mean_squared_error']])/len(scores_rf_diff['test_neg_mean_squared_error']))\n",
        "\n",
        "# mean absolute error\n",
        "print('Average MAE train data:', \n",
        "      sum([(-1 * x) for x in scores_rf_diff['train_neg_mean_absolute_error']])/len(scores_rf_diff['train_neg_mean_absolute_error']))\n",
        "print('Average MAE test data:', \n",
        "      sum([(-1 * x) for x in scores_rf_diff['test_neg_mean_absolute_error']])/len(scores_rf_diff['test_neg_mean_absolute_error']))\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average RMSE train data: 0.07879490507231\n",
            "Average RMSE test data: 0.21685512068445348\n",
            "Average MAE train data: 0.0486483235844493\n",
            "Average MAE test data: 0.1434120534800168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYr3NzrdfHTg",
        "outputId": "859cc360-16c8-4d66-e111-a15e6d14a61c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# getting indices for all folds from timeseriessplit\n",
        "X_train_over = defaultdict()  \n",
        "X_test_over = defaultdict()\n",
        "y_train_over = defaultdict()\n",
        "y_test_over = defaultdict()\n",
        "\n",
        "y_test_index = []\n",
        "y_train_index = []\n",
        "\n",
        "count = 0\n",
        "\n",
        "for train_index, test_index in time_split.split(X_diff):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    \n",
        "    y_test_index.append(test_index)\n",
        "    y_train_index.append(train_index)\n",
        "    \n",
        "    X_train, X_test = X_diff.iloc[train_index], X_diff.iloc[test_index]\n",
        "    y_train, y_test = (np.array(y_diff))[train_index], (np.array(y_diff))[test_index]\n",
        "    X_train_over[count] = X_train\n",
        "    X_test_over[count] = X_test\n",
        "    y_train_over[count] = y_train\n",
        "    y_test_over[count] = y_test\n",
        "    \n",
        "    count += 1"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [   0    1    2 ... 1108 1109 1110] TEST: [1111 1112 1113 ... 2216 2217 2218]\n",
            "TRAIN: [   0    1    2 ... 2216 2217 2218] TEST: [2219 2220 2221 ... 3324 3325 3326]\n",
            "TRAIN: [   0    1    2 ... 3324 3325 3326] TEST: [3327 3328 3329 ... 4432 4433 4434]\n",
            "TRAIN: [   0    1    2 ... 4432 4433 4434] TEST: [4435 4436 4437 ... 5540 5541 5542]\n",
            "TRAIN: [   0    1    2 ... 5540 5541 5542] TEST: [5543 5544 5545 ... 6648 6649 6650]\n",
            "TRAIN: [   0    1    2 ... 6648 6649 6650] TEST: [6651 6652 6653 ... 7756 7757 7758]\n",
            "TRAIN: [   0    1    2 ... 7756 7757 7758] TEST: [7759 7760 7761 ... 8864 8865 8866]\n",
            "TRAIN: [   0    1    2 ... 8864 8865 8866] TEST: [8867 8868 8869 ... 9972 9973 9974]\n",
            "TRAIN: [   0    1    2 ... 9972 9973 9974] TEST: [ 9975  9976  9977 ... 11080 11081 11082]\n",
            "TRAIN: [    0     1     2 ... 11080 11081 11082] TEST: [11083 11084 11085 ... 12188 12189 12190]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQj1qFLJfHTi"
      },
      "source": [
        "def split_predict_test(X_train, y_train, X_test, y_test, split):\n",
        "    \n",
        "    pipeline_part = pipeline.fit(X_train[split], y_train[split].ravel())\n",
        "    prediction_test = pipeline_part.predict(X_test[split])\n",
        "    #print('transformed RMSE:', np.sqrt(mean_squared_error(y_test[split], prediction_test)))\n",
        "        \n",
        "    # what are the predictions in absolute terms\n",
        "    y_test_pred_log = prediction_test + y_log.iloc[y_test_index[split]]['Trip_Count']\n",
        "    y_test_pred = np.exp(y_test_pred_log) - 2\n",
        "\n",
        "    # what are the actual values in absolute terms\n",
        "    y_test_true_log = y_test[split] + y_log.iloc[y_test_index[split]]\n",
        "    y_test_true = np.exp(y_test_true_log) - 2 # minus 2 because we added 1 each when doing the log function earlier\n",
        "\n",
        "    # what is the rmse, rmsle and mae\n",
        "    #print('actual RMSE:', np.sqrt(mean_squared_error(y_test_true, y_test_pred)))\n",
        "    #print('actual RMSLE:', np.sqrt(mean_squared_log_error(y_test_true, y_test_pred)))\n",
        "    #print('actual MAE:', mean_absolute_error(y_test_true, y_test_pred))\n",
        "    \n",
        "    rmse = np.sqrt(mean_squared_error(y_test_true, y_test_pred))\n",
        "    rmsle = np.sqrt(mean_squared_log_error(y_test_true, y_test_pred))\n",
        "    mae = mean_absolute_error(y_test_true, y_test_pred)\n",
        "    \n",
        "    list_scores = []\n",
        "    list_scores.extend([rmse, rmsle, mae])\n",
        "    \n",
        "    return list_scores\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82-eo2eXfHTj"
      },
      "source": [
        "def split_predict_train(X_train, y_train, X_test, y_test, split):\n",
        "    \n",
        "    pipeline_part = pipeline.fit(X_train[split], y_train[split].ravel())\n",
        "    prediction_train = pipeline_part.predict(X_train[split])\n",
        "    #print('transformed RMSE:', np.sqrt(mean_squared_error(y_test[split], prediction_test)))\n",
        "        \n",
        "    # what are the predictions in absolute terms\n",
        "    y_train_pred_log = prediction_train + y_log.iloc[y_train_index[split]]['Trip_Count']\n",
        "    y_train_pred = np.exp(y_train_pred_log) - 2\n",
        "\n",
        "    # what are the actual values in absolute terms\n",
        "    y_train_true_log = y_train[split] + y_log.iloc[y_train_index[split]]\n",
        "    y_train_true = np.exp(y_train_true_log) - 2 # minus 2 because we added 1 each when doing the log function earlier\n",
        "\n",
        "    # what is the rmse, rmsle and mae\n",
        "    #print('actual RMSE:', np.sqrt(mean_squared_error(y_test_true, y_test_pred)))\n",
        "    #print('actual RMSLE:', np.sqrt(mean_squared_log_error(y_test_true, y_test_pred)))\n",
        "    #print('actual MAE:', mean_absolute_error(y_test_true, y_test_pred))\n",
        "    \n",
        "    rmse = np.sqrt(mean_squared_error(y_train_true, y_train_pred))\n",
        "    rmsle = np.sqrt(mean_squared_log_error(y_train_true, y_train_pred))\n",
        "    mae = mean_absolute_error(y_train_true, y_train_pred)\n",
        "    \n",
        "    list_scores = []\n",
        "    list_scores.extend([rmse, rmsle, mae])\n",
        "    \n",
        "    return list_scores"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS2EYtzBfHTk"
      },
      "source": [
        "#### 4.4. Random Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOpp6yGwfHTl"
      },
      "source": [
        "##### doing run through with untransformed y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-wAUv2wfHTl"
      },
      "source": [
        "# initializing the model which is a Random Forest model and uses default hyperparameters\n",
        "model_rf = RandomForestRegressor(bootstrap=False, random_state=15)\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkNMo33gfHTm",
        "outputId": "2ba54829-4f55-4f27-be06-692a6c90fb93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "# creating and fitting the ML pipeline\n",
        "trend_features = ['Trip_Count_mean'+str(window), 'Trip_Count_std'+str(window), 'Trip_Count_t-1']\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n",
        "], remainder='passthrough')\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n",
        "    ('model', model_rf)\n",
        "])\n",
        "\n",
        "pipeline_rf = pipeline.fit(X, y)\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-873ad1c1e7c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m ])\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mpipeline_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \"\"\"\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2019-03-09'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn1EjppEfHTn"
      },
      "source": [
        "# creating a timeseries split of the datasets\n",
        "time_split = TimeSeriesSplit(n_splits=10)\n",
        "\n",
        "# doing cross validation on the chunks of data and calculating scores\n",
        "scores_rf = cross_validate(pipeline_rf, X, y, cv=time_split,\n",
        "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n",
        "                                  'neg_mean_squared_log_error'],\n",
        "                         return_train_score=True, n_jobs=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5nALXlNfHUX"
      },
      "source": [
        "# root mean squared error\n",
        "print('Random Forests: Average RMSE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_rf['train_neg_mean_squared_error']])/len(scores_rf['train_neg_mean_squared_error']))\n",
        "print('Random Forests: Average RMSE test data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_rf['test_neg_mean_squared_error']])/len(scores_rf['test_neg_mean_squared_error']))\n",
        "\n",
        "# mean absolute error\n",
        "print('Random Forests: Average MAE train data:', \n",
        "      sum([(-1 * x) for x in scores_rf['train_neg_mean_absolute_error']])/len(scores_rf['train_neg_mean_absolute_error']))\n",
        "print('Random Forests: Average MAE test data:', \n",
        "      sum([(-1 * x) for x in scores_rf['test_neg_mean_absolute_error']])/len(scores_rf['test_neg_mean_absolute_error']))\n",
        "\n",
        "# root mean squared log error\n",
        "print('Random Forests: Average RMSLE train data:', \n",
        "      sum([(-1 * x) for x in scores_rf['train_neg_mean_squared_log_error']])/len(scores_rf['train_neg_mean_squared_log_error']))\n",
        "print('Random Forests: Average RMSLE test data:', \n",
        "      sum([(-1 * x) for x in scores_rf['test_neg_mean_squared_log_error']])/len(scores_rf['test_neg_mean_squared_log_error']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkNNY3h8fHUZ"
      },
      "source": [
        "I will use randomizedsearch and gridsearch to tune my hyperparameters for the Random Forest model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da0q8uGmfHUZ"
      },
      "source": [
        "# number of trees in random forest\n",
        "n_estimators = randint(1, 2000)\n",
        "# maximum number of features included in the model\n",
        "max_features = randint(1, 20)\n",
        "# maximum number of levels in tree\n",
        "max_depth = randint(1,10)\n",
        "# minimum number of samples required to split a node\n",
        "min_samples_leaf = randint(1, 10)\n",
        "\n",
        "# create the random grid\n",
        "random_grid_rf = {'model__n_estimators': n_estimators,\n",
        "                   'model__max_depth': max_depth,\n",
        "                   'model__min_samples_leaf': min_samples_leaf,\n",
        "                   'model__max_features': max_features}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L86v-pmzfHUe"
      },
      "source": [
        "I'm using a try and except argument to load an already tuned model or, if none is available, to run randomsearch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "b5Mu7G60fHUe"
      },
      "source": [
        "# check the start time\n",
        "start_time = datetime.now()\n",
        "print(start_time)\n",
        "\n",
        "# instantiate and fit the randomizedsearch class to the random parameters\n",
        "rs_rf = RandomizedSearchCV(pipeline_rf, \n",
        "                        param_distributions=random_grid_rf, \n",
        "                        scoring='neg_mean_squared_error', \n",
        "                        n_jobs=-1,\n",
        "                        cv=time_split,\n",
        "                        n_iter = 5,\n",
        "                        verbose=10,\n",
        "                        random_state=49)\n",
        "rs_rf = rs_rf.fit(X, y)\n",
        "\n",
        "# print the total running time\n",
        "end_time = datetime.now()\n",
        "print('Total running time:', end_time-start_time)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fazu3_wifHUl"
      },
      "source": [
        "# Saving the best RandomForest model\n",
        "pickle.dump(rs_rf.best_estimator_, open('randomforest.sav', 'wb'))\n",
        "\n",
        "# change the keys of the best_params dictionary to allow it to be used for the final model\n",
        "fix_best_params_rf = {key[7:]: val for key, val in rs_rf.best_params_.items()}\n",
        "\n",
        "print(fix_best_params_rf)\n",
        "\n",
        "# fit the randomforestregressor with the best_params as hyperparameters\n",
        "model_rf_rs = RandomForestRegressor(**fix_best_params_rf, bootstrap=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL64evhyfHUm"
      },
      "source": [
        "# creating and fitting the ML pipeline\n",
        "trend_features = ['Trip_Count_mean'+str(window), 'Trip_Count_std'+str(window), 'Trip_Count_t-1']\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n",
        "], remainder='passthrough')\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n",
        "    ('model', model_rf_rs)\n",
        "])\n",
        "\n",
        "# fitting x and y to pipeline\n",
        "pipeline_rf_rs = pipeline.fit(X, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb0gfEyNfHUy"
      },
      "source": [
        "# doing cross validation on the chunks of data and calculating scores\n",
        "scores_rf = cross_validate(pipeline_rf_rs, X, y, cv=time_split,\n",
        "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n",
        "                                  'neg_mean_squared_log_error'],\n",
        "                         return_train_score=True, n_jobs=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5wPBuLkfHU2"
      },
      "source": [
        "# root mean squared error\n",
        "print('Random Forests: Average RMSE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_rf['train_neg_mean_squared_error']])/len(scores_rf['train_neg_mean_squared_error']))\n",
        "print('Random Forests: Average RMSE test data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_rf['test_neg_mean_squared_error']])/len(scores_rf['test_neg_mean_squared_error']))\n",
        "\n",
        "# absolute mean error\n",
        "print('Random Forests: Average MAE train data:', \n",
        "      sum([(-1 * x) for x in scores_rf['train_neg_mean_absolute_error']])/len(scores_rf['train_neg_mean_absolute_error']))\n",
        "print('Random Forests: Average MAE test data:', \n",
        "      sum([(-1 * x) for x in scores_rf['test_neg_mean_absolute_error']])/len(scores_rf['test_neg_mean_absolute_error']))\n",
        "\n",
        "# root mean squared log error\n",
        "print('Random Forests: Average RMSLE train data:', \n",
        "      sum([(-1 * x) for x in scores_rf['train_neg_mean_squared_log_error']])/len(scores_rf['train_neg_mean_squared_log_error']))\n",
        "print('Random Forests: Average RMSLE test data:', \n",
        "      sum([(-1 * x) for x in scores_rf['test_neg_mean_squared_log_error']])/len(scores_rf['test_neg_mean_squared_log_error']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2iby6cSfHU5"
      },
      "source": [
        "# assign lists of parameters to be used in gridsearch\n",
        "param_grid_rf = {'model__max_depth': [8, 9],\n",
        "                 'model__max_features': [18, 19],\n",
        "                 'model__min_samples_leaf': [5, 6],\n",
        "                 'model__n_estimators': [1926, 1930]\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv2qLnIjfHU6"
      },
      "source": [
        "# use gridsearch to search around the randomizedsearch best parameters and further improve the model\n",
        "gs_rf = GridSearchCV(pipeline_rf, \n",
        "                  param_grid=param_grid_rf, \n",
        "                  scoring='neg_mean_squared_error', \n",
        "                  verbose = 10,\n",
        "                  n_jobs=-1, \n",
        "                  cv=time_split)\n",
        "gs_rf = gs_rf.fit(X, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDLGPxmMfHU9"
      },
      "source": [
        "# Saving the best RandomForest model\n",
        "pickle.dump(gs_rf.best_estimator_, open('randomforest.sav', 'wb'))\n",
        "\n",
        "# change the keys of the best_params dictionary to allow it to be used for the final model\n",
        "fix_best_params_rf = {key[7:]: val for key, val in gs_rf.best_params_.items()}\n",
        "\n",
        "print(fix_best_params_rf)\n",
        "\n",
        "# fit the randomforestregressor with the best_params as hyperparameters\n",
        "model_rf_tuned = RandomForestRegressor(**fix_best_params_rf, bootstrap=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBTMMjuZfHU_"
      },
      "source": [
        "# creating and fitting the ML pipeline\n",
        "trend_features = ['Trip_Count_mean'+str(window), 'Trip_Count_std'+str(window), 'Trip_Count_t-1']\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n",
        "], remainder='passthrough')\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n",
        "    ('model', model_rf_tuned)\n",
        "])\n",
        "\n",
        "# fitting x and y to pipeline\n",
        "pipeline_rf_tuned = pipeline.fit(X, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEywb5jBfHVB"
      },
      "source": [
        "# check the start time\n",
        "start_time = datetime.now()\n",
        "\n",
        "# doing cross validation on the chunks of data and calculating scores\n",
        "scores_rf_tuned = cross_validate(pipeline_rf_tuned, X, y, cv=time_split,\n",
        "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n",
        "                                  'neg_mean_squared_log_error'],\n",
        "                         return_train_score=True, n_jobs=-1)\n",
        "\n",
        "# print the total running time\n",
        "end_time = datetime.now()\n",
        "print('Total cross validation time for random forests:', (end_time-start_time).total_seconds())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcZ_WPTOfHVC"
      },
      "source": [
        "# root mean squared error\n",
        "print('Random Forests: Average RMSE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_rf_tuned['train_neg_mean_squared_error']])/len(scores_rf_tuned['train_neg_mean_squared_error']))\n",
        "print('Random Forests: Average RMSE test data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_rf_tuned['test_neg_mean_squared_error']])/len(scores_rf_tuned['test_neg_mean_squared_error']))\n",
        "\n",
        "# mean absolute error\n",
        "print('Random Forests: Average MAE train data:', \n",
        "      sum([(-1 * x) for x in scores_rf_tuned['train_neg_mean_absolute_error']])/len(scores_rf_tuned['train_neg_mean_absolute_error']))\n",
        "print('Random Forests: Average MAE test data:', \n",
        "      sum([(-1 * x) for x in scores_rf_tuned['test_neg_mean_absolute_error']])/len(scores_rf_tuned['test_neg_mean_absolute_error']))\n",
        "\n",
        "# root mean squared log error\n",
        "print('Random Forests: Average RMSLE train data:', \n",
        "      sum([(-1 * x) for x in scores_rf_tuned['train_neg_mean_squared_log_error']])/len(scores_rf_tuned['train_neg_mean_squared_log_error']))\n",
        "print('Random Forests: Average RMSLE test data:', \n",
        "      sum([(-1 * x) for x in scores_rf_tuned['test_neg_mean_squared_log_error']])/len(scores_rf_tuned['test_neg_mean_squared_log_error']))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "1lx9z3NyfHVD"
      },
      "source": [
        "# plot feature importance\n",
        "plt.figure(figsize=[12,9])\n",
        "importances = list(pipeline_rf_tuned.steps[1][1].feature_importances_)\n",
        "\n",
        "imp_dict = {key: val for key, val in zip(list(X.columns), importances)}\n",
        "sorted_feats = sorted(imp_dict.items(), key=lambda x:x[1], reverse=True)\n",
        "x_val = [x[0] for x in sorted_feats]\n",
        "y_val = [x[1] for x in sorted_feats]\n",
        "\n",
        "sb.barplot(y_val, x_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvy2WIdVfHXd"
      },
      "source": [
        "##### running model with differenced y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k0iOfpKfHXd"
      },
      "source": [
        "# get the final scores for the differenced data\n",
        "split = 10\n",
        "all_scores_test = []\n",
        "all_scores_train = []\n",
        "\n",
        "for i in range(split):\n",
        "    scores_test = split_predict_test(X_train_over, y_train_over, X_test_over, y_test_over, i)\n",
        "    all_scores_test.append(scores_test)\n",
        "    scores_train = split_predict_train(X_train_over, y_train_over, X_test_over, y_test_over, i)\n",
        "    all_scores_train.append(scores_train)\n",
        "    \n",
        "rmse_test = []\n",
        "rmsle_test = []\n",
        "mae_test = []\n",
        "rmse_train = []\n",
        "rmsle_train = []\n",
        "mae_train = []\n",
        "\n",
        "for vals in all_scores_test:\n",
        "    rmse_test.append(vals[0])\n",
        "    rmsle_test.append(vals[1])\n",
        "    mae_test.append(vals[2])\n",
        "    \n",
        "for vals in all_scores_train:\n",
        "    rmse_train.append(vals[0])\n",
        "    rmsle_train.append(vals[1])\n",
        "    mae_train.append(vals[2])\n",
        "    \n",
        "print('Overall Test RMSE:', sum(rmse_test)/split)\n",
        "print('Overall Test RMSLE:', sum(rmsle_test)/split)\n",
        "print('Overall Test MAE:', sum(mae_test)/split)\n",
        "\n",
        "print('Overall Train RMSE:', sum(rmse_train)/split)\n",
        "print('Overall Train RMSLE:', sum(rmsle_train)/split)\n",
        "print('Overall Train MAE:', sum(mae_train)/split)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCYkG7yBfHYf"
      },
      "source": [
        "##### Removing some features based on feature importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4lSXPqWfHYf"
      },
      "source": [
        "# how many features are in the dataset currently:\n",
        "len(x_val)\n",
        "weekday_feats = ['weekday_3', 'weekday_1', 'weekday_2', 'weekday_4', 'weekday_5', 'weekday_6']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSTmSrjWfHYl"
      },
      "source": [
        "# check how the model performance without certain features that are very unimportant\n",
        "X_feat_imp = X.drop(columns=weekday_feats)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CxSCR5zfHYn"
      },
      "source": [
        "# creating and fitting the ML pipeline\n",
        "trend_features = ['Trip_Count_mean'+str(window), 'Trip_Count_std'+str(window)]#, 'Trip_Count_t-1']\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n",
        "], remainder='passthrough')\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n",
        "    ('model', model_rf_tuned)\n",
        "])\n",
        "\n",
        "# fitting x and y to pipeline\n",
        "pipeline_rf_tuned = pipeline.fit(X_feat_imp, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXlVKcLkfHYo"
      },
      "source": [
        "# doing cross validation on the chunks of data and calculating scores\n",
        "scores_rf_tuned = cross_validate(pipeline_rf_tuned, X_feat_imp, y, cv=time_split,\n",
        "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n",
        "                                  'neg_mean_squared_log_error'],\n",
        "                         return_train_score=True, n_jobs=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGsUzQW-fHYr"
      },
      "source": [
        "# root mean squared error\n",
        "print('Random Forests: Average RMSE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_rf_tuned['train_neg_mean_squared_error']])/len(scores_rf_tuned['train_neg_mean_squared_error']))\n",
        "print('Random Forests: Average RMSLE test data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_rf_tuned['test_neg_mean_squared_error']])/len(scores_rf_tuned['test_neg_mean_squared_error']))\n",
        "\n",
        "# mean absolute error\n",
        "print('Random Forests: Average MAE train data:', \n",
        "      sum([(-1 * x) for x in scores_rf_tuned['train_neg_mean_absolute_error']])/len(scores_rf_tuned['train_neg_mean_absolute_error']))\n",
        "print('Random Forests: Average MAE test data:', \n",
        "      sum([(-1 * x) for x in scores_rf_tuned['test_neg_mean_absolute_error']])/len(scores_rf_tuned['test_neg_mean_absolute_error']))\n",
        "\n",
        "# root mean squared log error\n",
        "print('Random Forests: Average RMSLE train data:', \n",
        "      sum([(-1 * x) for x in scores_rf_tuned['train_neg_mean_squared_log_error']])/len(scores_rf_tuned['train_neg_mean_squared_log_error']))\n",
        "print('Random Forests: Average RMSLE test data:', \n",
        "      sum([(-1 * x) for x in scores_rf_tuned['test_neg_mean_squared_log_error']])/len(scores_rf_tuned['test_neg_mean_squared_log_error']))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJZiEg70fHYv"
      },
      "source": [
        "Removing seemingly unimportant features does not lead to a better result. Unimportant features are simply not considered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M7bNuNtfHYw"
      },
      "source": [
        "#### 4.2. AdaBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84PI2ONtfHYw"
      },
      "source": [
        "##### running AdaBoost with undifference y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV4AcD0WfHYw"
      },
      "source": [
        "# initializing AdaBoost model with default hyperparameters\n",
        "model_ada = AdaBoostRegressor(random_state=42, base_estimator=DecisionTreeRegressor())\n",
        "\n",
        "# creating and fitting the ML pipeline\n",
        "trend_features = ['Trip_Count_mean'+str(window), 'Trip_Count_std'+str(window), 'Trip_Count_t-1']\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n",
        "], remainder='passthrough')\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n",
        "    ('model', model_ada)\n",
        "])\n",
        "\n",
        "pipeline_ada = pipeline.fit(X, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gsvyd6UjfHYx"
      },
      "source": [
        "# creating a timeseries split of the datasets\n",
        "time_split = TimeSeriesSplit(n_splits=10)\n",
        "\n",
        "# doing cross validation on the chunks of data and calculating scores\n",
        "scores_ada = cross_validate(pipeline_ada, X, y, cv=time_split,\n",
        "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n",
        "                                  'neg_mean_squared_log_error'],\n",
        "                         return_train_score=True, n_jobs=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfJgMvfxfHYy"
      },
      "source": [
        "# root mean squared error\n",
        "print('AdaBoost: Average RMSE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_ada['train_neg_mean_squared_error']])/len(scores_ada['train_neg_mean_squared_error']))\n",
        "print('AdaBoost: Average RMSE test data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_ada['test_neg_mean_squared_error']])/len(scores_ada['test_neg_mean_squared_error']))\n",
        "\n",
        "# mean absolute error\n",
        "print('AdaBoost: Average MAE train data:', \n",
        "      sum([(-1 * x) for x in scores_ada['train_neg_mean_absolute_error']])/len(scores_ada['train_neg_mean_absolute_error']))\n",
        "print('AdaBoost: Average MAE test data:', \n",
        "      sum([(-1 * x) for x in scores_ada['test_neg_mean_absolute_error']])/len(scores_ada['test_neg_mean_absolute_error']))\n",
        "\n",
        "# root mean squared log error\n",
        "print('AdaBoost: Average RMSLE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_ada['train_neg_mean_squared_log_error']])/len(scores_ada['train_neg_mean_squared_log_error']))\n",
        "print('AdaBoost: Average RMSLE test data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_ada['test_neg_mean_squared_log_error']])/len(scores_ada['test_neg_mean_squared_log_error']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIbduX8-fHY3"
      },
      "source": [
        "Tuning the hyperparameters of AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsqoasjSfHY3"
      },
      "source": [
        "# number of estimators in AdaBoost model\n",
        "n_estimators = randint(100, 1000)\n",
        "# learning rate\n",
        "learning_rate = uniform(0.001, 0.05)\n",
        "# max_depth\n",
        "max_depth = randint(1, 8)\n",
        "\n",
        "# Create the random grid\n",
        "random_grid_ada = {'model__n_estimators': n_estimators,\n",
        "                   'model__learning_rate': learning_rate,\n",
        "                   'model__base_estimator__max_depth': max_depth}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5SKilp5fHY5"
      },
      "source": [
        "# check the start time\n",
        "start_time = datetime.now()\n",
        "print(start_time)\n",
        "\n",
        "# instantiate and fit the randomizedsearch class to the random parameters\n",
        "rs_ada = RandomizedSearchCV(pipeline_ada, \n",
        "                        param_distributions=random_grid_ada, \n",
        "                        scoring='neg_mean_squared_error', \n",
        "                        n_jobs=-1,\n",
        "                        cv=time_split,\n",
        "                        n_iter = 5,\n",
        "                        verbose=10,\n",
        "                        random_state=40)\n",
        "rs_ada = rs_ada.fit(X, y)\n",
        "\n",
        "# print the total running time\n",
        "end_time = datetime.now()\n",
        "print('Total running time:', end_time-start_time)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Jbg3SXWwfHY6"
      },
      "source": [
        "# Saving the best RandomForest model\n",
        "pickle.dump(rs_ada.best_estimator_, open('adaboost.sav', 'wb'))\n",
        "\n",
        "# change the keys of the best_params dictionary to allow it to be used for the final model\n",
        "fix_best_params_ada = {key[7:]: val for key, val in rs_ada.best_params_.items()}\n",
        "\n",
        "print(fix_best_params_ada)\n",
        "max_depth = fix_best_params_ada['base_estimator__max_depth']\n",
        "\n",
        "# fit the randomforestregressor with the best_params as hyperparameters\n",
        "model_ada_tuned = AdaBoostRegressor(DecisionTreeRegressor(max_depth=max_depth),\n",
        "                                    learning_rate=fix_best_params_ada['learning_rate'], \n",
        "                                    n_estimators=fix_best_params_ada['n_estimators'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDNivp-MfHY7"
      },
      "source": [
        "# creating and fitting the ML pipeline\n",
        "trend_features = ['Trip_Count_mean'+str(window), 'Trip_Count_std'+str(window), 'Trip_Count_t-1']\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n",
        "], remainder='passthrough')\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n",
        "    ('model', model_ada_tuned)\n",
        "])\n",
        "\n",
        "pipeline_ada_tuned = pipeline.fit(X, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPYCp6HIfHY8"
      },
      "source": [
        "# creating a timeseries split of the datasets\n",
        "time_split = TimeSeriesSplit(n_splits=10)\n",
        "\n",
        "# doing cross validation on the chunks of data and calculating scores\n",
        "scores_ada_tuned = cross_validate(pipeline_ada_tuned, X, y, cv=time_split,\n",
        "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n",
        "                                  'neg_mean_squared_log_error'],\n",
        "                         return_train_score=True, n_jobs=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG9Uee0HfHZD"
      },
      "source": [
        "# root mean squared error\n",
        "print('AdaBoost: Average RMSE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['train_neg_mean_squared_error']])/len(scores_ada_tuned['train_neg_mean_squared_error']))\n",
        "print('AdaBoost: Average RMSE test data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['test_neg_mean_squared_error']])/len(scores_ada_tuned['test_neg_mean_squared_error']))\n",
        "\n",
        "# mean absolute error\n",
        "print('AdaBoost: Average MAE train data:', \n",
        "      sum([(-1 * x) for x in scores_ada_tuned['train_neg_mean_absolute_error']])/len(scores_ada_tuned['train_neg_mean_absolute_error']))\n",
        "print('AdaBoost: Average MAE test data:', \n",
        "      sum([(-1 * x) for x in scores_ada_tuned['test_neg_mean_absolute_error']])/len(scores_ada_tuned['test_neg_mean_absolute_error']))\n",
        "\n",
        "# root mean squared log error\n",
        "print('AdaBoost: Average RMSLE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['train_neg_mean_squared_log_error']])/len(scores_ada_tuned['train_neg_mean_squared_log_error']))\n",
        "print('AdaBoost: Average RMSLEtest data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['test_neg_mean_squared_log_error']])/len(scores_ada_tuned['test_neg_mean_squared_log_error']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg9Ip-QcfHZE"
      },
      "source": [
        "# specify some values for hyperparameters around the values chosen by randomizedsearch\n",
        "grid_param_ada = {'model__learning_rate': [0.003, 0.004, 0.0332],\n",
        "                  'model__n_estimators': [100, 265],\n",
        "                  'model__base_estimator__max_depth': [5, 6, 7]}\n",
        "\n",
        "\n",
        "# use gridsearch to search around the randomizedsearch best parameters and further improve the model\n",
        "gs_ada = GridSearchCV(pipeline_ada, \n",
        "                  param_grid=grid_param_ada, \n",
        "                  scoring='neg_mean_squared_error',\n",
        "                  verbose = 10,\n",
        "                  n_jobs=-1, \n",
        "                  cv=time_split)\n",
        "\n",
        "gs_ada = gs_ada.fit(X, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCTXoptBfHZF"
      },
      "source": [
        "# Saving the best RandomForest model\n",
        "#pickle.dump(gs_ada.best_estimator_, open('adaboost.sav', 'wb'))\n",
        "\n",
        "# change the keys of the best_params dictionary to allow it to be used for the final model\n",
        "fix_best_params_ada = {key[7:]: val for key, val in gs_ada.best_params_.items()}\n",
        "\n",
        "print(fix_best_params_ada)\n",
        "\n",
        "max_depth = fix_best_params_ada['base_estimator__max_depth']\n",
        "\n",
        "# fit the randomforestregressor with the best_params as hyperparameters\n",
        "model_ada_tuned = AdaBoostRegressor(DecisionTreeRegressor(max_depth=max_depth),\n",
        "                                    learning_rate=fix_best_params_ada['learning_rate'], \n",
        "                                    n_estimators=fix_best_params_ada['n_estimators'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUbZeKWYfHZG"
      },
      "source": [
        "# creating and fitting the ML pipeline\n",
        "trend_features = ['Trip_Count_mean'+str(window), 'Trip_Count_std'+str(window), 'Trip_Count_t-1']\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n",
        "], remainder='passthrough')\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n",
        "    ('model', model_ada_tuned)\n",
        "])\n",
        "\n",
        "pipeline_ada_tuned = pipeline.fit(X, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG1L-MExfHZM"
      },
      "source": [
        "# creating a timeseries split of the datasets\n",
        "time_split = TimeSeriesSplit(n_splits=10)\n",
        "\n",
        "# check the start time\n",
        "start_time = datetime.now()\n",
        "\n",
        "# doing cross validation on the chunks of data and calculating scores\n",
        "scores_ada_tuned = cross_validate(pipeline_ada_tuned, X, y, cv=time_split,\n",
        "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n",
        "                                  'neg_mean_squared_log_error'],\n",
        "                         return_train_score=True, n_jobs=-1)\n",
        "\n",
        "# print the total running time\n",
        "end_time = datetime.now()\n",
        "print('Total cross validation time for AdaBoost:', (end_time-start_time).total_seconds())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-si_HyrfHZO"
      },
      "source": [
        "# root mean squared error\n",
        "print('AdaBoost: Average RMSE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['train_neg_mean_squared_error']])/len(scores_ada_tuned['train_neg_mean_squared_error']))\n",
        "print('AdaBoost: Average RMSE test data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['test_neg_mean_squared_error']])/len(scores_ada_tuned['test_neg_mean_squared_error']))\n",
        "\n",
        "# mean average error\n",
        "print('AdaBoost: Average MAE train data:', \n",
        "      sum([(-1 * x) for x in scores_ada_tuned['train_neg_mean_absolute_error']])/len(scores_ada_tuned['train_neg_mean_absolute_error']))\n",
        "print('AdaBoost: Average MAE test data:', \n",
        "      sum([(-1 * x) for x in scores_ada_tuned['test_neg_mean_absolute_error']])/len(scores_ada_tuned['test_neg_mean_absolute_error']))\n",
        "\n",
        "# root mean squared log error\n",
        "print('AdaBoost: Average RMSLE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['train_neg_mean_squared_log_error']])/len(scores_ada_tuned['train_neg_mean_squared_log_error']))\n",
        "print('AdaBoost: Average RMSLE test data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['test_neg_mean_squared_log_error']])/len(scores_ada_tuned['test_neg_mean_squared_log_error']))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNbJYVs9fHZS"
      },
      "source": [
        "# plot feature importance\n",
        "plt.figure(figsize=[12,9])\n",
        "importances = list(pipeline_ada_tuned.steps[1][1].feature_importances_)\n",
        "\n",
        "imp_dict = {key: val for key, val in zip(list(X.columns), importances)}\n",
        "sorted_feats = sorted(imp_dict.items(), key=lambda x:x[1], reverse=True)\n",
        "x_val = [x[0] for x in sorted_feats]\n",
        "y_val = [x[1] for x in sorted_feats]\n",
        "\n",
        "#importances.sort(reverse=True)\n",
        "sb.barplot(y_val, x_val)\n",
        "#importances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjkbKk2rfHZS"
      },
      "source": [
        "##### running AdaBoost with differenced y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-YKnZBAfHZT"
      },
      "source": [
        "# get the final scores for the differenced data\n",
        "split = 10\n",
        "all_scores_test = []\n",
        "all_scores_train = []\n",
        "\n",
        "for i in range(split):\n",
        "    scores_test = split_predict_test(X_train_over, y_train_over, X_test_over, y_test_over, i)\n",
        "    all_scores_test.append(scores_test)\n",
        "    scores_train = split_predict_train(X_train_over, y_train_over, X_test_over, y_test_over, i)\n",
        "    all_scores_train.append(scores_train)\n",
        "    \n",
        "rmse_test = []\n",
        "rmsle_test = []\n",
        "mae_test = []\n",
        "rmse_train = []\n",
        "rmsle_train = []\n",
        "mae_train = []\n",
        "\n",
        "for vals in all_scores_test:\n",
        "    rmse_test.append(vals[0])\n",
        "    rmsle_test.append(vals[1])\n",
        "    mae_test.append(vals[2])\n",
        "    \n",
        "for vals in all_scores_train:\n",
        "    rmse_train.append(vals[0])\n",
        "    rmsle_train.append(vals[1])\n",
        "    mae_train.append(vals[2])\n",
        "    \n",
        "print('Overall Test RMSE:', sum(rmse_test)/split)\n",
        "print('Overall Test RMSLE:', sum(rmsle_test)/split)\n",
        "print('Overall Test MAE:', sum(mae_test)/split)\n",
        "\n",
        "print('Overall Train RMSE:', sum(rmse_train)/split)\n",
        "print('Overall Train RMSLE:', sum(rmsle_train)/split)\n",
        "print('Overall Train MAE:', sum(mae_train)/split)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KTSe07sfHZU"
      },
      "source": [
        "##### checking how certain features influence the overall performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3esiGOjvfHZV"
      },
      "source": [
        "# check how the model performance without certain features that are very unimportant\n",
        "X_feat_imp = X.drop(columns=['weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFlw5yyJfHZW"
      },
      "source": [
        "# creating and fitting the ML pipeline\n",
        "trend_features = ['Trip_Count_mean'+str(window), 'Trip_Count_std'+str(window)]#, 'Trip_Count_t-1']\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n",
        "], remainder='passthrough')\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n",
        "    ('model', model_ada_tuned)\n",
        "])\n",
        "\n",
        "pipeline_ada_tuned = pipeline.fit(X_feat_imp, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qedR39WfHZX"
      },
      "source": [
        "# creating a timeseries split of the datasets\n",
        "time_split = TimeSeriesSplit(n_splits=10)\n",
        "\n",
        "# doing cross validation on the chunks of data and calculating scores\n",
        "scores_ada_tuned = cross_validate(pipeline_ada_tuned, X_feat_imp, y, cv=time_split,\n",
        "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n",
        "                                  'neg_mean_squared_log_error'],\n",
        "                         return_train_score=True, n_jobs=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "AHToaAxJfHZY"
      },
      "source": [
        "# root mean squared error\n",
        "print('AdaBoost: Average RMSE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['train_neg_mean_squared_error']])/len(scores_ada_tuned['train_neg_mean_squared_error']))\n",
        "print('AdaBoost: Average RMSE test data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['test_neg_mean_squared_error']])/len(scores_ada_tuned['test_neg_mean_squared_error']))\n",
        "\n",
        "# mean average error\n",
        "print('AdaBoost: Average MAE train data:', \n",
        "      sum([(-1 * x) for x in scores_ada_tuned['train_neg_mean_absolute_error']])/len(scores_ada_tuned['train_neg_mean_absolute_error']))\n",
        "print('AdaBoost: Average MAE test data:', \n",
        "      sum([(-1 * x) for x in scores_ada_tuned['test_neg_mean_absolute_error']])/len(scores_ada_tuned['test_neg_mean_absolute_error']))\n",
        "\n",
        "# root mean squared log error\n",
        "print('AdaBoost: Average RMSLE train data:', \n",
        "      sum([(-1 * x) for x in scores_ada_tuned['train_neg_mean_squared_log_error']])/len(scores_ada_tuned['train_neg_mean_squared_log_error']))\n",
        "print('AdaBoost: Average RMSLE test data:', \n",
        "      sum([(-1 * x) for x in scores_ada_tuned['test_neg_mean_squared_log_error']])/len(scores_ada_tuned['test_neg_mean_squared_log_error']))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MakQlHnwfHZZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmz0XFGmfHZZ"
      },
      "source": [
        "#### 4.3. XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMEUeFxJfHZa"
      },
      "source": [
        "##### running XGBoost with undifferenced y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTjdTlZnfHZa"
      },
      "source": [
        "# initializing XGBoost model with default hyperparameters\n",
        "model_xgb = xgb.XGBRegressor(random_state=42)\n",
        "\n",
        "# creating and fitting the ML pipeline\n",
        "trend_features = ['Trip_Count_mean'+str(window), 'Trip_Count_std'+str(window), 'Trip_Count_t-1']\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n",
        "], remainder='passthrough')\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n",
        "    ('model', model_xgb)\n",
        "])\n",
        "\n",
        "pipeline_xgb = pipeline.fit(X, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7O-BjeYfHZe"
      },
      "source": [
        "# creating a timeseries split of the datasets\n",
        "time_split = TimeSeriesSplit(n_splits=10)\n",
        "\n",
        "# doing cross validation on the chunks of data and calculating scores\n",
        "scores_xgb = cross_validate(pipeline_xgb, X, y, cv=time_split,\n",
        "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n",
        "                                  'neg_mean_squared_log_error'],\n",
        "                         return_train_score=True, n_jobs=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5Ty1vYhfHZf"
      },
      "source": [
        "# root mean squared error\n",
        "print('XGBoost: Average RMSE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_xgb['train_neg_mean_squared_error']])/len(scores_xgb['train_neg_mean_squared_error']))\n",
        "print('XGBoost: Average RMSE test data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_xgb['test_neg_mean_squared_error']])/len(scores_xgb['test_neg_mean_squared_error']))\n",
        "\n",
        "# mean absolute error\n",
        "print('XGBoost: Average MAE train data:', \n",
        "      sum([(-1 * x) for x in scores_xgb['train_neg_mean_absolute_error']])/len(scores_xgb['train_neg_mean_absolute_error']))\n",
        "print('XGBoost: Average MAE test data:', \n",
        "      sum([(-1 * x) for x in scores_xgb['test_neg_mean_absolute_error']])/len(scores_xgb['test_neg_mean_absolute_error']))\n",
        "\n",
        "# root mean squared log error\n",
        "print('XGBoost: Average RMSLE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_xgb['train_neg_mean_squared_log_error']])/len(scores_xgb['train_neg_mean_squared_log_error']))\n",
        "print('XGBoost: Average RMSLE test data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_xgb['test_neg_mean_squared_log_error']])/len(scores_xgb['test_neg_mean_squared_log_error']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS_nn5XVfHZg"
      },
      "source": [
        "# alpha\n",
        "alpha = uniform(0.2, 0.5)\n",
        "# learning rate\n",
        "learning_rate = uniform(0, 1)\n",
        "# colsample_bytree\n",
        "colsample_bytree = uniform(0, 1)\n",
        "# max depth\n",
        "n_estimators = randint(300, 1000)\n",
        "# min child weight\n",
        "min_child_weight = randint(5, 10)\n",
        "# max_depth\n",
        "max_depth = randint(1, 5)\n",
        "# subsample\n",
        "# subsample = uniform(0, 1)\n",
        "\n",
        "# Create the random grid\n",
        "random_grid_xgb = {'model__n_estimators': n_estimators,\n",
        "                   'model__learning_rate': learning_rate,\n",
        "                   'model__reg_alpha': alpha,\n",
        "                   'model__colsample_bytree': colsample_bytree,\n",
        "                   'model__min_child_weight': min_child_weight,\n",
        "                   'model__max_depth': max_depth}\n",
        "                   #'model__subsample': subsample}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3XYiHbRfHZh"
      },
      "source": [
        "# check the start time\n",
        "start_time = datetime.now()\n",
        "print(start_time)\n",
        "\n",
        "# instantiate and fit the randomizedsearch class to the random parameters\n",
        "rs_xgb = RandomizedSearchCV(pipeline_xgb, \n",
        "                        param_distributions=random_grid_xgb, \n",
        "                        scoring='neg_mean_squared_error', \n",
        "                        n_jobs=-1,\n",
        "                        cv=time_split,\n",
        "                        n_iter = 10,\n",
        "                        verbose=10,\n",
        "                        random_state=10)\n",
        "rs_xgb = rs_xgb.fit(X, y)\n",
        "\n",
        "# print the total running time\n",
        "end_time = datetime.now()\n",
        "print('Total running time:', end_time-start_time)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbrO45C_fHZk"
      },
      "source": [
        "# Saving the best RandomForest model\n",
        "pickle.dump(rs_xgb.best_estimator_, open('xgboost.sav', 'wb'))\n",
        "\n",
        "# change the keys of the best_params dictionary to allow it to be used for the final model\n",
        "fix_best_params_xgb = {key[7:]: val for key, val in rs_xgb.best_params_.items()}\n",
        "\n",
        "print(fix_best_params_xgb)\n",
        "\n",
        "# fit the randomforestregressor with the best_params as hyperparameters\n",
        "model_xgb_rs = xgb.XGBRegressor(**fix_best_params_xgb)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pX8CBfAfHZl"
      },
      "source": [
        "# creating and fitting the ML pipeline\n",
        "trend_features = ['Trip_Count_mean'+str(window), 'Trip_Count_std'+str(window), 'Trip_Count_t-1']\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n",
        "], remainder='passthrough')\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n",
        "    ('model', model_xgb_rs)\n",
        "])\n",
        "\n",
        "pipeline_xgb_rs = pipeline.fit(X, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBSHvtbffHZv"
      },
      "source": [
        "# creating a timeseries split of the datasets\n",
        "time_split = TimeSeriesSplit(n_splits=10)\n",
        "\n",
        "# doing cross validation on the chunks of data and calculating scores\n",
        "scores_xgb_rs = cross_validate(pipeline_xgb_rs, X, y, cv=time_split,\n",
        "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n",
        "                                  'neg_mean_squared_log_error'],\n",
        "                         return_train_score=True, n_jobs=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYssC9uffHZw"
      },
      "source": [
        "# root mean squared error\n",
        "print('XGBoost: Average RMSE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_xgb_rs['train_neg_mean_squared_error']])/len(scores_xgb_rs['train_neg_mean_squared_error']))\n",
        "print('XGBoost: Average RMSE test data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_xgb_rs['test_neg_mean_squared_error']])/len(scores_xgb_rs['test_neg_mean_squared_error']))\n",
        "\n",
        "# mean absolute error\n",
        "print('XGBoost: Average MAE train data:', \n",
        "      sum([(-1 * x) for x in scores_xgb_rs['train_neg_mean_absolute_error']])/len(scores_xgb_rs['train_neg_mean_absolute_error']))\n",
        "print('XGBoost: Average MAE test data:', \n",
        "      sum([(-1 * x) for x in scores_xgb_rs['test_neg_mean_absolute_error']])/len(scores_xgb_rs['test_neg_mean_absolute_error']))\n",
        "\n",
        "# root mean squared log error\n",
        "print('XGBoost: Average RMSLE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_xgb_rs['train_neg_mean_squared_log_error']])/len(scores_xgb_rs['train_neg_mean_squared_log_error']))\n",
        "print('XGBoost: Average RMSLE test data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_xgb_rs['test_neg_mean_squared_log_error']])/len(scores_xgb_rs['test_neg_mean_squared_log_error']))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "IIA8TELafHZ4"
      },
      "source": [
        "# specify some values for hyperparameters around the values chosen by randomizedsearch\n",
        "grid_param_xgb = {'model__n_estimators': [664, 650],\n",
        "                   'model__learning_rate': [0.35, 0.04],\n",
        "                   'model__alpha': [0.636, 0.6],\n",
        "                   'model__colsample_bytree': [0.85, 0.9],\n",
        "                   'model__min_child_weight': [7, 8],\n",
        "                   'model__max_depth': [1, 2]}\n",
        "                   #'model__subsample': [0.98, 0.90]}\n",
        "\n",
        "# use gridsearch to search around the randomizedsearch best parameters and further improve the model\n",
        "gs_xgb = GridSearchCV(pipeline_xgb, \n",
        "                  param_grid=grid_param_xgb, \n",
        "                  scoring='neg_mean_squared_error', \n",
        "                  verbose = 10,\n",
        "                  n_jobs=-1, \n",
        "                  cv=time_split)\n",
        "\n",
        "gs_xgb = gs_xgb.fit(X, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LG752yHfHZ5"
      },
      "source": [
        "# Saving the best XGB model\n",
        "pickle.dump(gs_xgb.best_estimator_, open('xgboost.sav', 'wb'))\n",
        "\n",
        "# change the keys of the best_params dictionary to allow it to be used for the final model\n",
        "fix_best_params_xgb = {key[7:]: val for key, val in gs_xgb.best_params_.items()}\n",
        "\n",
        "print(fix_best_params_xgb)\n",
        "\n",
        "# fit the randomforestregressor with the best_params as hyperparameters\n",
        "model_xgb_tuned = xgb.XGBRegressor(**fix_best_params_xgb)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Iy4nGtpcfHZ9"
      },
      "source": [
        "# creating and fitting the ML pipeline\n",
        "trend_features = ['Trip_Count_mean'+str(window), 'Trip_Count_std'+str(window), 'Trip_Count_t-1']\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n",
        "], remainder='passthrough')\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n",
        "    ('model', model_xgb_tuned)\n",
        "])\n",
        "\n",
        "pipeline_xgb_tuned = pipeline.fit(X, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqjcgVOKfHaB"
      },
      "source": [
        "# creating a timeseries split of the datasets\n",
        "time_split = TimeSeriesSplit(n_splits=10)\n",
        "\n",
        "# check the start time\n",
        "start_time = datetime.now()\n",
        "\n",
        "# doing cross validation on the chunks of data and calculating scores\n",
        "scores_xgb_tuned = cross_validate(pipeline_xgb_tuned, X, y, cv=time_split,\n",
        "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n",
        "                                  'neg_mean_squared_log_error'],\n",
        "                         return_train_score=True, n_jobs=-1)\n",
        "\n",
        "# print the total running time\n",
        "end_time = datetime.now()\n",
        "print('Total cross validation time for XGBBoost:', (end_time-start_time).total_seconds())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptZlX3ETfHaN"
      },
      "source": [
        "# root mean squared error\n",
        "print('XGBoost: Average RMSE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_xgb_tuned['train_neg_mean_squared_error']])/len(scores_xgb_tuned['train_neg_mean_squared_error']))\n",
        "print('XGBoost: Average RMSE test data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_xgb_tuned['test_neg_mean_squared_error']])/len(scores_xgb_tuned['test_neg_mean_squared_error']))\n",
        "\n",
        "# mean absolute error\n",
        "print('XGBoost: Average MAE train data:', \n",
        "      sum([(-1 * x) for x in scores_xgb_tuned['train_neg_mean_absolute_error']])/len(scores_xgb_tuned['train_neg_mean_absolute_error']))\n",
        "print('XGBoost: Average MAE test data:', \n",
        "      sum([(-1 * x) for x in scores_xgb_tuned['test_neg_mean_absolute_error']])/len(scores_xgb_tuned['test_neg_mean_absolute_error']))\n",
        "\n",
        "# root mean squared log error\n",
        "print('XGBoost: Average RMSLE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_xgb_tuned['train_neg_mean_squared_log_error']])/len(scores_xgb_tuned['train_neg_mean_squared_log_error']))\n",
        "print('XGBoost: Average RMSLE test data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_xgb_tuned['test_neg_mean_squared_log_error']])/len(scores_xgb_tuned['test_neg_mean_squared_log_error']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV8RSeMcfHaP"
      },
      "source": [
        "# map the feature importances to the feature names\n",
        "xgb_feats = X.columns\n",
        "xgb_feats_dict = defaultdict()\n",
        "count = 0\n",
        "\n",
        "for item in xgb_feats:\n",
        "    xgb_feats_dict['f'+str(count)] = item\n",
        "    count += 1\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpy1uJAkfHbI"
      },
      "source": [
        "# Plot feature importance\n",
        "fig, ax = plt.subplots(figsize=(20, 15))\n",
        "xgb.plot_importance(model_xgb_tuned, importance_type='weight', ax=ax)\n",
        "\n",
        "# rename the y axis labels to the actual feature names\n",
        "locs, labels = plt.yticks()\n",
        "\n",
        "new_names = []\n",
        "for item in labels:\n",
        "    for key, name in xgb_feats_dict.items():\n",
        "        if key == item.get_text():\n",
        "            new_names.append(name)\n",
        "\n",
        "ax.set_yticklabels(new_names);\n",
        "            \n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gjAniNofHbg"
      },
      "source": [
        "##### running XGBoost with differenced y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Is96_gKsfHbi"
      },
      "source": [
        "# get the final scores for the differenced data\n",
        "split = 10\n",
        "all_scores_test = []\n",
        "all_scores_train = []\n",
        "\n",
        "for i in range(split):\n",
        "    scores_test = split_predict_test(X_train_over, y_train_over, X_test_over, y_test_over, i)\n",
        "    all_scores_test.append(scores_test)\n",
        "    scores_train = split_predict_train(X_train_over, y_train_over, X_test_over, y_test_over, i)\n",
        "    all_scores_train.append(scores_train)\n",
        "    \n",
        "rmse_test = []\n",
        "rmsle_test = []\n",
        "mae_test = []\n",
        "rmse_train = []\n",
        "rmsle_train = []\n",
        "mae_train = []\n",
        "\n",
        "for vals in all_scores_test:\n",
        "    rmse_test.append(vals[0])\n",
        "    rmsle_test.append(vals[1])\n",
        "    mae_test.append(vals[2])\n",
        "    \n",
        "for vals in all_scores_train:\n",
        "    rmse_train.append(vals[0])\n",
        "    rmsle_train.append(vals[1])\n",
        "    mae_train.append(vals[2])\n",
        "    \n",
        "print('Overall Test RMSE:', sum(rmse_test)/split)\n",
        "print('Overall Test RMSLE:', sum(rmsle_test)/split)\n",
        "print('Overall Test MAE:', sum(mae_test)/split)\n",
        "\n",
        "print('Overall Train RMSE:', sum(rmse_train)/split)\n",
        "print('Overall Train RMSLE:', sum(rmsle_train)/split)\n",
        "print('Overall Train MAE:', sum(mae_train)/split)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDpXNxSIfHbj"
      },
      "source": [
        "##### checking how certain features influence the overall performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_F3Ep0QfHbj"
      },
      "source": [
        "# check how the model performance without certain features that are very unimportant\n",
        "X_feat_imp = X.drop(columns=['weekday_1', 'weekday_3', 'weekday_4', 'weekday_6',\n",
        "                             'hot', 'cold', 'cool', 'warm'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DULDEIgnfHby"
      },
      "source": [
        "# creating and fitting the ML pipeline\n",
        "trend_features = ['Trip_Count_mean'+str(window), 'Trip_Count_std'+str(window), 'Trip_Count_t-1']\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n",
        "], remainder='passthrough')\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n",
        "    ('model', model_xgb_tuned)\n",
        "])\n",
        "\n",
        "pipeline_xgb_tuned = pipeline.fit(X_feat_imp, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJdAvgG5fHb2"
      },
      "source": [
        "# creating a timeseries split of the datasets\n",
        "time_split = TimeSeriesSplit(n_splits=10)\n",
        "\n",
        "# doing cross validation on the chunks of data and calculating scores\n",
        "scores_xgb_tuned = cross_validate(pipeline_xgb_tuned, X_feat_imp, y, cv=time_split,\n",
        "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error',\n",
        "                                  'neg_mean_squared_log_error'],\n",
        "                         return_train_score=True, n_jobs=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKkbNpa7fHb3"
      },
      "source": [
        "# root mean squared error\n",
        "print('XGBoost: Average RMSE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_xgb_tuned['train_neg_mean_squared_error']])/len(scores_xgb_tuned['train_neg_mean_squared_error']))\n",
        "print('XGBoost: Average RMSE test data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_xgb_tuned['test_neg_mean_squared_error']])/len(scores_xgb_tuned['test_neg_mean_squared_error']))\n",
        "\n",
        "# mean absolute error\n",
        "print('XGBoost: Average MAE train data:', \n",
        "      sum([(-1 * x) for x in scores_xgb_tuned['train_neg_mean_absolute_error']])/len(scores_xgb_tuned['train_neg_mean_absolute_error']))\n",
        "print('XGBoost: Average MAE test data:', \n",
        "      sum([(-1 * x) for x in scores_xgb_tuned['test_neg_mean_absolute_error']])/len(scores_xgb_tuned['test_neg_mean_absolute_error']))\n",
        "\n",
        "# root mean squared log error\n",
        "print('XGBoost: Average RMSLE train data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_xgb_tuned['train_neg_mean_squared_log_error']])/len(scores_xgb_tuned['train_neg_mean_squared_log_error']))\n",
        "print('XGBoost: Average RMSLE test data:', \n",
        "      sum([np.sqrt(-1 * x) for x in scores_xgb_tuned['test_neg_mean_squared_log_error']])/len(scores_xgb_tuned['test_neg_mean_squared_log_error']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc4YX78AfHb6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9sRMXsLfHb7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLIJOeDlfHb9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}